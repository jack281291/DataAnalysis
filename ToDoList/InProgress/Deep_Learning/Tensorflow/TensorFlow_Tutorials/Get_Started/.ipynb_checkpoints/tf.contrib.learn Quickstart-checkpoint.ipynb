{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> tf.contrib.learn Quickstart <center>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow’s high-level machine learning API (tf.contrib.learn) makes it easy to configure, train, and evaluate a variety of machine learning models. In this tutorial, you’ll use tf.contrib.learn to construct a neural network classifier and train it on the Iris data set to predict flower species based on sepal/petal geometry. You'll write code to perform the following five steps:\n",
    "\n",
    "    -Load CSVs containing Iris training/test data into a TensorFlow Dataset\n",
    "    -Construct a neural network classifier\n",
    "    -Fit the model using the training data\n",
    "    -Evaluate the accuracy of the model\n",
    "    -Classify new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Load the Iris CSV data to TensorFlow\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training and test sets into Datasets using the load_csv_with_header() method in learn.datasets.base. The load_csv_with_header() method takes three required arguments:\n",
    "\n",
    "    -filename, which takes the filepath to the CSV file\n",
    "    -target_dtype, which takes the numpy datatype of the dataset's target value.\n",
    "    -features_dtype, which takes the numpy datatype of the dataset's feature values.\n",
    "\n",
    "Here, the target (the value you're training the model to predict) is flower species, which is an integer from 0–2, so the appropriate numpy datatype is np.int:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IRIS_TRAINING = \"/home/antonio/Desktop/TensorFlow_Tutorials/Data/Iris/iris_training.csv\"\n",
    "IRIS_TEST = \"/home/antonio/Desktop/TensorFlow_Tutorials/Data/Iris/iris_test.csv\"\n",
    "\n",
    "# Load datasets.\n",
    "training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "    filename=IRIS_TRAINING,\n",
    "    target_dtype=np.int,\n",
    "    features_dtype=np.float32)\n",
    "test_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "    filename=IRIS_TEST,\n",
    "    target_dtype=np.int,\n",
    "    features_dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets in tf.contrib.learn are named tuples; you can access feature data and target values via the data and target fields. Here, **training_set.data and training_set.target contain the feature data and target values for the training set, respectively, and test_set.data and test_set.target contain feature data and target values for the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 0, 0, 0, 0, 2, 1, 0, 1, 1, 0, 0, 2, 1, 2, 2, 2, 0, 2, 2, 0,\n",
       "       2, 2, 0, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 0, 0, 2,\n",
       "       0, 2, 0, 2, 0, 1, 1, 0, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 0, 2, 2,\n",
       "       0, 0, 1, 0, 2, 2, 0, 1, 1, 1, 2, 0, 1, 1, 1, 2, 0, 1, 1, 1, 0, 2, 1,\n",
       "       0, 0, 2, 0, 0, 2, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 2, 1, 0, 2, 0,\n",
       "       1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.4000001 ,  2.79999995,  5.5999999 ,  2.20000005],\n",
       "       [ 5.        ,  2.29999995,  3.29999995,  1.        ],\n",
       "       [ 4.9000001 ,  2.5       ,  4.5       ,  1.70000005],\n",
       "       [ 4.9000001 ,  3.0999999 ,  1.5       ,  0.1       ],\n",
       "       [ 5.69999981,  3.79999995,  1.70000005,  0.30000001],\n",
       "       [ 4.4000001 ,  3.20000005,  1.29999995,  0.2       ],\n",
       "       [ 5.4000001 ,  3.4000001 ,  1.5       ,  0.40000001],\n",
       "       [ 6.9000001 ,  3.0999999 ,  5.0999999 ,  2.29999995],\n",
       "       [ 6.69999981,  3.0999999 ,  4.4000001 ,  1.39999998],\n",
       "       [ 5.0999999 ,  3.70000005,  1.5       ,  0.40000001],\n",
       "       [ 5.19999981,  2.70000005,  3.9000001 ,  1.39999998],\n",
       "       [ 6.9000001 ,  3.0999999 ,  4.9000001 ,  1.5       ],\n",
       "       [ 5.80000019,  4.        ,  1.20000005,  0.2       ],\n",
       "       [ 5.4000001 ,  3.9000001 ,  1.70000005,  0.40000001],\n",
       "       [ 7.69999981,  3.79999995,  6.69999981,  2.20000005],\n",
       "       [ 6.30000019,  3.29999995,  4.69999981,  1.60000002],\n",
       "       [ 6.80000019,  3.20000005,  5.9000001 ,  2.29999995],\n",
       "       [ 7.5999999 ,  3.        ,  6.5999999 ,  2.0999999 ],\n",
       "       [ 6.4000001 ,  3.20000005,  5.30000019,  2.29999995],\n",
       "       [ 5.69999981,  4.4000001 ,  1.5       ,  0.40000001],\n",
       "       [ 6.69999981,  3.29999995,  5.69999981,  2.0999999 ],\n",
       "       [ 6.4000001 ,  2.79999995,  5.5999999 ,  2.0999999 ],\n",
       "       [ 5.4000001 ,  3.9000001 ,  1.29999995,  0.40000001],\n",
       "       [ 6.0999999 ,  2.5999999 ,  5.5999999 ,  1.39999998],\n",
       "       [ 7.19999981,  3.        ,  5.80000019,  1.60000002],\n",
       "       [ 5.19999981,  3.5       ,  1.5       ,  0.2       ],\n",
       "       [ 5.80000019,  2.5999999 ,  4.        ,  1.20000005],\n",
       "       [ 5.9000001 ,  3.        ,  5.0999999 ,  1.79999995],\n",
       "       [ 5.4000001 ,  3.        ,  4.5       ,  1.5       ],\n",
       "       [ 6.69999981,  3.        ,  5.        ,  1.70000005],\n",
       "       [ 6.30000019,  2.29999995,  4.4000001 ,  1.29999995],\n",
       "       [ 5.0999999 ,  2.5       ,  3.        ,  1.10000002],\n",
       "       [ 6.4000001 ,  3.20000005,  4.5       ,  1.5       ],\n",
       "       [ 6.80000019,  3.        ,  5.5       ,  2.0999999 ],\n",
       "       [ 6.19999981,  2.79999995,  4.80000019,  1.79999995],\n",
       "       [ 6.9000001 ,  3.20000005,  5.69999981,  2.29999995],\n",
       "       [ 6.5       ,  3.20000005,  5.0999999 ,  2.        ],\n",
       "       [ 5.80000019,  2.79999995,  5.0999999 ,  2.4000001 ],\n",
       "       [ 5.0999999 ,  3.79999995,  1.5       ,  0.30000001],\n",
       "       [ 4.80000019,  3.        ,  1.39999998,  0.30000001],\n",
       "       [ 7.9000001 ,  3.79999995,  6.4000001 ,  2.        ],\n",
       "       [ 5.80000019,  2.70000005,  5.0999999 ,  1.89999998],\n",
       "       [ 6.69999981,  3.        ,  5.19999981,  2.29999995],\n",
       "       [ 5.0999999 ,  3.79999995,  1.89999998,  0.40000001],\n",
       "       [ 4.69999981,  3.20000005,  1.60000002,  0.2       ],\n",
       "       [ 6.        ,  2.20000005,  5.        ,  1.5       ],\n",
       "       [ 4.80000019,  3.4000001 ,  1.60000002,  0.2       ],\n",
       "       [ 7.69999981,  2.5999999 ,  6.9000001 ,  2.29999995],\n",
       "       [ 4.5999999 ,  3.5999999 ,  1.        ,  0.2       ],\n",
       "       [ 7.19999981,  3.20000005,  6.        ,  1.79999995],\n",
       "       [ 5.        ,  3.29999995,  1.39999998,  0.2       ],\n",
       "       [ 6.5999999 ,  3.        ,  4.4000001 ,  1.39999998],\n",
       "       [ 6.0999999 ,  2.79999995,  4.        ,  1.29999995],\n",
       "       [ 5.        ,  3.20000005,  1.20000005,  0.2       ],\n",
       "       [ 7.        ,  3.20000005,  4.69999981,  1.39999998],\n",
       "       [ 6.        ,  3.        ,  4.80000019,  1.79999995],\n",
       "       [ 7.4000001 ,  2.79999995,  6.0999999 ,  1.89999998],\n",
       "       [ 5.80000019,  2.70000005,  5.0999999 ,  1.89999998],\n",
       "       [ 6.19999981,  3.4000001 ,  5.4000001 ,  2.29999995],\n",
       "       [ 5.        ,  2.        ,  3.5       ,  1.        ],\n",
       "       [ 5.5999999 ,  2.5       ,  3.9000001 ,  1.10000002],\n",
       "       [ 6.69999981,  3.0999999 ,  5.5999999 ,  2.4000001 ],\n",
       "       [ 6.30000019,  2.5       ,  5.        ,  1.89999998],\n",
       "       [ 6.4000001 ,  3.0999999 ,  5.5       ,  1.79999995],\n",
       "       [ 6.19999981,  2.20000005,  4.5       ,  1.5       ],\n",
       "       [ 7.30000019,  2.9000001 ,  6.30000019,  1.79999995],\n",
       "       [ 4.4000001 ,  3.        ,  1.29999995,  0.2       ],\n",
       "       [ 7.19999981,  3.5999999 ,  6.0999999 ,  2.5       ],\n",
       "       [ 6.5       ,  3.        ,  5.5       ,  1.79999995],\n",
       "       [ 5.        ,  3.4000001 ,  1.5       ,  0.2       ],\n",
       "       [ 4.69999981,  3.20000005,  1.29999995,  0.2       ],\n",
       "       [ 6.5999999 ,  2.9000001 ,  4.5999999 ,  1.29999995],\n",
       "       [ 5.5       ,  3.5       ,  1.29999995,  0.2       ],\n",
       "       [ 7.69999981,  3.        ,  6.0999999 ,  2.29999995],\n",
       "       [ 6.0999999 ,  3.        ,  4.9000001 ,  1.79999995],\n",
       "       [ 4.9000001 ,  3.0999999 ,  1.5       ,  0.1       ],\n",
       "       [ 5.5       ,  2.4000001 ,  3.79999995,  1.10000002],\n",
       "       [ 5.69999981,  2.9000001 ,  4.19999981,  1.29999995],\n",
       "       [ 6.        ,  2.9000001 ,  4.5       ,  1.5       ],\n",
       "       [ 6.4000001 ,  2.70000005,  5.30000019,  1.89999998],\n",
       "       [ 5.4000001 ,  3.70000005,  1.5       ,  0.2       ],\n",
       "       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.39999998],\n",
       "       [ 6.5       ,  2.79999995,  4.5999999 ,  1.5       ],\n",
       "       [ 5.5999999 ,  2.70000005,  4.19999981,  1.29999995],\n",
       "       [ 6.30000019,  3.4000001 ,  5.5999999 ,  2.4000001 ],\n",
       "       [ 4.9000001 ,  3.0999999 ,  1.5       ,  0.1       ],\n",
       "       [ 6.80000019,  2.79999995,  4.80000019,  1.39999998],\n",
       "       [ 5.69999981,  2.79999995,  4.5       ,  1.29999995],\n",
       "       [ 6.        ,  2.70000005,  5.0999999 ,  1.60000002],\n",
       "       [ 5.        ,  3.5       ,  1.29999995,  0.30000001],\n",
       "       [ 6.5       ,  3.        ,  5.19999981,  2.        ],\n",
       "       [ 6.0999999 ,  2.79999995,  4.69999981,  1.20000005],\n",
       "       [ 5.0999999 ,  3.5       ,  1.39999998,  0.30000001],\n",
       "       [ 4.5999999 ,  3.0999999 ,  1.5       ,  0.2       ],\n",
       "       [ 6.5       ,  3.        ,  5.80000019,  2.20000005],\n",
       "       [ 4.5999999 ,  3.4000001 ,  1.39999998,  0.30000001],\n",
       "       [ 4.5999999 ,  3.20000005,  1.39999998,  0.2       ],\n",
       "       [ 7.69999981,  2.79999995,  6.69999981,  2.        ],\n",
       "       [ 5.9000001 ,  3.20000005,  4.80000019,  1.79999995],\n",
       "       [ 5.0999999 ,  3.79999995,  1.60000002,  0.2       ],\n",
       "       [ 4.9000001 ,  3.        ,  1.39999998,  0.2       ],\n",
       "       [ 4.9000001 ,  2.4000001 ,  3.29999995,  1.        ],\n",
       "       [ 4.5       ,  2.29999995,  1.29999995,  0.30000001],\n",
       "       [ 5.80000019,  2.70000005,  4.0999999 ,  1.        ],\n",
       "       [ 5.        ,  3.4000001 ,  1.60000002,  0.40000001],\n",
       "       [ 5.19999981,  3.4000001 ,  1.39999998,  0.2       ],\n",
       "       [ 5.30000019,  3.70000005,  1.5       ,  0.2       ],\n",
       "       [ 5.        ,  3.5999999 ,  1.39999998,  0.2       ],\n",
       "       [ 5.5999999 ,  2.9000001 ,  3.5999999 ,  1.29999995],\n",
       "       [ 4.80000019,  3.0999999 ,  1.60000002,  0.2       ],\n",
       "       [ 6.30000019,  2.70000005,  4.9000001 ,  1.79999995],\n",
       "       [ 5.69999981,  2.79999995,  4.0999999 ,  1.29999995],\n",
       "       [ 5.        ,  3.        ,  1.60000002,  0.2       ],\n",
       "       [ 6.30000019,  3.29999995,  6.        ,  2.5       ],\n",
       "       [ 5.        ,  3.5       ,  1.60000002,  0.60000002],\n",
       "       [ 5.5       ,  2.5999999 ,  4.4000001 ,  1.20000005],\n",
       "       [ 5.69999981,  3.        ,  4.19999981,  1.20000005],\n",
       "       [ 4.4000001 ,  2.9000001 ,  1.39999998,  0.2       ],\n",
       "       [ 4.80000019,  3.        ,  1.39999998,  0.1       ],\n",
       "       [ 5.5       ,  2.4000001 ,  3.70000005,  1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Construct a Deep Neural Network Classifier\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.contrib.learn offers a variety of predefined models, called **Estimators**, which you can use \"out of the box\" to run training and evaluation operations on your data. Here, you'll configure a Deep Neural Network Classifier model to fit the Iris data. Using tf.contrib.learn, you can instantiate your **tf.contrib.learn.DNNClassifier** with just a couple lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_num_ps_replicas': 0, '_tf_random_seed': None, '_environment': 'local', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_task_type': None, '_is_chief': True, '_save_summary_steps': 100, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', '_task_id': 0, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa8b5172438>, '_master': ''}\n"
     ]
    }
   ],
   "source": [
    "# Specify that all features have real-value data\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n",
    "\n",
    "# Build 3 layer DNN with 10, 20, 10 units respectively.\n",
    "classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\n",
    "                                            hidden_units=[10, 20, 10],\n",
    "                                            n_classes=3,\n",
    "                                            model_dir=\"/tmp/iris_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above first defines the model's feature columns, which specify the data type for the features in the data set. All the feature data is continuous, so tf.contrib.layers.real_valued_column is the appropriate function to use to construct the feature columns. There are four features in the data set (sepal width, sepal height, petal width, and petal height), so accordingly dimension must be set to 4 to hold all the data.\n",
    "\n",
    "Then, the code creates a DNNClassifier model using the following arguments:\n",
    "\n",
    "    -feature_columns=feature_columns. The set of feature columns defined above.\n",
    "    -hidden_units=[10, 20, 10]. Three hidden layers, containing 10, 20, and 10 neurons, respectively.\n",
    "    -n_classes=3. Three target classes, representing the three Iris species.\n",
    "    -model_dir=/tmp/iris_model. The directory in which TensorFlow will save checkpoint data during model training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Describe the training input pipeline\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **tf.contrib.learn API uses input functions, which create the TensorFlow operations that generate data for the model. In this case, the data is small enough that it can be stored in tf.constant TensorFlow constants.** The following code produces the simplest possible input pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the test inputs\n",
    "def get_train_inputs():\n",
    "    x = tf.constant(training_set.data)\n",
    "    y = tf.constant(training_set.target)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Fit the DNNClassifier to the Iris Training Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've configured your DNN classifier model, you can fit it to the Iris training data using the fit method. Pass get_train_inputs as the input_fn, and the number of steps to train (here, 2000):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/antonio/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/iris_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.13434, step = 1\n",
      "INFO:tensorflow:global_step/sec: 1178.15\n",
      "INFO:tensorflow:loss = 0.397414, step = 101\n",
      "INFO:tensorflow:global_step/sec: 1316.22\n",
      "INFO:tensorflow:loss = 0.128143, step = 201\n",
      "INFO:tensorflow:global_step/sec: 1172.45\n",
      "INFO:tensorflow:loss = 0.0930441, step = 301\n",
      "INFO:tensorflow:global_step/sec: 1280.75\n",
      "INFO:tensorflow:loss = 0.0809191, step = 401\n",
      "INFO:tensorflow:global_step/sec: 1070.75\n",
      "INFO:tensorflow:loss = 0.0742832, step = 501\n",
      "INFO:tensorflow:global_step/sec: 1152.74\n",
      "INFO:tensorflow:loss = 0.0685734, step = 601\n",
      "INFO:tensorflow:global_step/sec: 524.514\n",
      "INFO:tensorflow:loss = 0.0652555, step = 701\n",
      "INFO:tensorflow:global_step/sec: 960.177\n",
      "INFO:tensorflow:loss = 0.0618733, step = 801\n",
      "INFO:tensorflow:global_step/sec: 1233.5\n",
      "INFO:tensorflow:loss = 0.0595477, step = 901\n",
      "INFO:tensorflow:global_step/sec: 1142.04\n",
      "INFO:tensorflow:loss = 0.0576326, step = 1001\n",
      "INFO:tensorflow:global_step/sec: 1292.86\n",
      "INFO:tensorflow:loss = 0.0560004, step = 1101\n",
      "INFO:tensorflow:global_step/sec: 1183.18\n",
      "INFO:tensorflow:loss = 0.0545785, step = 1201\n",
      "INFO:tensorflow:global_step/sec: 1120.42\n",
      "INFO:tensorflow:loss = 0.0532793, step = 1301\n",
      "INFO:tensorflow:global_step/sec: 1137.08\n",
      "INFO:tensorflow:loss = 0.0525458, step = 1401\n",
      "INFO:tensorflow:global_step/sec: 1139.46\n",
      "INFO:tensorflow:loss = 0.0514058, step = 1501\n",
      "INFO:tensorflow:global_step/sec: 1197.71\n",
      "INFO:tensorflow:loss = 0.0502937, step = 1601\n",
      "INFO:tensorflow:global_step/sec: 1260.08\n",
      "INFO:tensorflow:loss = 0.0492695, step = 1701\n",
      "INFO:tensorflow:global_step/sec: 1196.24\n",
      "INFO:tensorflow:loss = 0.0484379, step = 1801\n",
      "INFO:tensorflow:global_step/sec: 1220.51\n",
      "INFO:tensorflow:loss = 0.0475732, step = 1901\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /tmp/iris_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0469128.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(params={'hidden_units': [10, 20, 10], 'gradient_clip_norm': None, 'feature_columns': (_RealValuedColumn(column_name='', dimension=4, default_value=None, dtype=tf.float32, normalizer=None),), 'dropout': None, 'embedding_lr_multipliers': None, 'head': <tensorflow.contrib.learn.python.learn.estimators.head._MultiClassHead object at 0x7fa8b51723c8>, 'input_layer_min_slice_size': None, 'activation_fn': <function relu at 0x7fa8b8d47048>, 'optimizer': None})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model.\n",
    "classifier.fit(input_fn=get_train_inputs, steps=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state of the model is preserved in the classifier, which means you can train iteratively if you like. For example, the above is equivalent to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-221e14b2595e>:1: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-14-221e14b2595e>:1: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /home/antonio/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonio/.local/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py:247: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  equality = a == b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 2001 into /tmp/iris_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0468412, step = 2001\n",
      "INFO:tensorflow:global_step/sec: 737.971\n",
      "INFO:tensorflow:loss = 0.046167, step = 2101\n",
      "INFO:tensorflow:global_step/sec: 722.954\n",
      "INFO:tensorflow:loss = 0.0455046, step = 2201\n",
      "INFO:tensorflow:global_step/sec: 750.272\n",
      "INFO:tensorflow:loss = 0.0448742, step = 2301\n",
      "INFO:tensorflow:global_step/sec: 415.812\n",
      "INFO:tensorflow:loss = 0.0442847, step = 2401\n",
      "INFO:tensorflow:global_step/sec: 507.218\n",
      "INFO:tensorflow:loss = 0.0436814, step = 2501\n",
      "INFO:tensorflow:global_step/sec: 816.003\n",
      "INFO:tensorflow:loss = 0.0431349, step = 2601\n",
      "INFO:tensorflow:global_step/sec: 808.244\n",
      "INFO:tensorflow:loss = 0.042599, step = 2701\n",
      "INFO:tensorflow:global_step/sec: 762.429\n",
      "INFO:tensorflow:loss = 0.0422802, step = 2801\n",
      "INFO:tensorflow:global_step/sec: 813.036\n",
      "INFO:tensorflow:loss = 0.0416324, step = 2901\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into /tmp/iris_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0411142.\n",
      "WARNING:tensorflow:From <ipython-input-14-221e14b2595e>:2: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-14-221e14b2595e>:2: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /home/antonio/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 3001 into /tmp/iris_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0411596, step = 3001\n",
      "INFO:tensorflow:global_step/sec: 694.847\n",
      "INFO:tensorflow:loss = 0.0406415, step = 3101\n",
      "INFO:tensorflow:global_step/sec: 855.061\n",
      "INFO:tensorflow:loss = 0.0401541, step = 3201\n",
      "INFO:tensorflow:global_step/sec: 796.128\n",
      "INFO:tensorflow:loss = 0.039693, step = 3301\n",
      "INFO:tensorflow:global_step/sec: 717.543\n",
      "INFO:tensorflow:loss = 0.0392347, step = 3401\n",
      "INFO:tensorflow:global_step/sec: 759.145\n",
      "INFO:tensorflow:loss = 0.0388124, step = 3501\n",
      "INFO:tensorflow:global_step/sec: 683.327\n",
      "INFO:tensorflow:loss = 0.0383842, step = 3601\n",
      "INFO:tensorflow:global_step/sec: 731.505\n",
      "INFO:tensorflow:loss = 0.0378881, step = 3701\n",
      "INFO:tensorflow:global_step/sec: 703.586\n",
      "INFO:tensorflow:loss = 0.0374486, step = 3801\n",
      "INFO:tensorflow:global_step/sec: 678.276\n",
      "INFO:tensorflow:loss = 0.0370078, step = 3901\n",
      "INFO:tensorflow:Saving checkpoints for 4000 into /tmp/iris_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0365727.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(params={'hidden_units': [10, 20, 10], 'gradient_clip_norm': None, 'feature_columns': (_RealValuedColumn(column_name='', dimension=4, default_value=None, dtype=tf.float32, normalizer=None),), 'dropout': None, 'embedding_lr_multipliers': None, 'head': <tensorflow.contrib.learn.python.learn.estimators.head._MultiClassHead object at 0x7fa8b51723c8>, 'input_layer_min_slice_size': None, 'activation_fn': <function relu at 0x7fa8b8d47048>, 'optimizer': None})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x=training_set.data, y=training_set.target, steps=1000)\n",
    "classifier.fit(x=training_set.data, y=training_set.target, steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Evaluate Model Accuracy\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/antonio/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-04-05-21:08:23\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-04-05-21:08:23\n",
      "INFO:tensorflow:Saving dict for global step 4000: accuracy = 0.966667, auc = 0.998333, global_step = 4000, loss = 0.0801882\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "\n",
      "Test Accuracy: 0.966667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the test inputs\n",
    "def get_test_inputs():\n",
    "    x = tf.constant(test_set.data)\n",
    "    y = tf.constant(test_set.target)\n",
    "    return x, y\n",
    "\n",
    "# Evaluate accuracy.\n",
    "accuracy_score = classifier.evaluate(input_fn=get_test_inputs,\n",
    "                                     steps=1)[\"accuracy\"]\n",
    "\n",
    "print(\"\\nTest Accuracy: {0:f}\\n\".format(accuracy_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Classify New Samples\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the estimator's predict() method to classify new samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Samples, Class Predictions:    [1, 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classify two new flower samples.\n",
    "def new_samples():\n",
    "    return np.array(\n",
    "        [[6.4, 3.2, 4.5, 1.5],\n",
    "         [5.8, 3.1, 5.0, 1.7]], dtype=np.float32)\n",
    "\n",
    "predictions = list(classifier.predict(input_fn=new_samples))\n",
    "\n",
    "print(\n",
    "    \"New Samples, Class Predictions:    {}\\n\"\n",
    "    .format(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
