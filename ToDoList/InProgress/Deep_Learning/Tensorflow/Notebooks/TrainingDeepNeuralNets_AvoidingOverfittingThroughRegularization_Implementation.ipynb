{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_outputs = 5\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train1 = mnist.train.images[mnist.train.labels < 5]\n",
    "y_train1 = mnist.train.labels[mnist.train.labels < 5]\n",
    "X_valid1 = mnist.validation.images[mnist.validation.labels < 5]\n",
    "y_valid1 = mnist.validation.labels[mnist.validation.labels < 5]\n",
    "X_test1 = mnist.test.images[mnist.test.labels < 5]\n",
    "y_test1 = mnist.test.labels[mnist.test.labels < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.135001\tBest loss: 0.135001\tAccuracy: 96.21%\n",
      "1\tValidation loss: 0.110945\tBest loss: 0.110945\tAccuracy: 96.68%\n",
      "2\tValidation loss: 0.099125\tBest loss: 0.099125\tAccuracy: 96.91%\n",
      "3\tValidation loss: 0.090837\tBest loss: 0.090837\tAccuracy: 97.26%\n",
      "4\tValidation loss: 0.084038\tBest loss: 0.084038\tAccuracy: 97.50%\n",
      "5\tValidation loss: 0.077733\tBest loss: 0.077733\tAccuracy: 97.69%\n",
      "6\tValidation loss: 0.075994\tBest loss: 0.075994\tAccuracy: 97.73%\n",
      "7\tValidation loss: 0.069468\tBest loss: 0.069468\tAccuracy: 97.85%\n",
      "8\tValidation loss: 0.065393\tBest loss: 0.065393\tAccuracy: 98.20%\n",
      "9\tValidation loss: 0.062352\tBest loss: 0.062352\tAccuracy: 98.24%\n",
      "10\tValidation loss: 0.060715\tBest loss: 0.060715\tAccuracy: 98.40%\n",
      "11\tValidation loss: 0.058149\tBest loss: 0.058149\tAccuracy: 98.32%\n",
      "12\tValidation loss: 0.054784\tBest loss: 0.054784\tAccuracy: 98.36%\n",
      "13\tValidation loss: 0.053220\tBest loss: 0.053220\tAccuracy: 98.36%\n",
      "14\tValidation loss: 0.050953\tBest loss: 0.050953\tAccuracy: 98.44%\n",
      "15\tValidation loss: 0.049342\tBest loss: 0.049342\tAccuracy: 98.36%\n",
      "16\tValidation loss: 0.047583\tBest loss: 0.047583\tAccuracy: 98.48%\n",
      "17\tValidation loss: 0.047188\tBest loss: 0.047188\tAccuracy: 98.48%\n",
      "18\tValidation loss: 0.045428\tBest loss: 0.045428\tAccuracy: 98.51%\n",
      "19\tValidation loss: 0.044016\tBest loss: 0.044016\tAccuracy: 98.51%\n",
      "20\tValidation loss: 0.043196\tBest loss: 0.043196\tAccuracy: 98.48%\n",
      "21\tValidation loss: 0.042496\tBest loss: 0.042496\tAccuracy: 98.48%\n",
      "22\tValidation loss: 0.041947\tBest loss: 0.041947\tAccuracy: 98.44%\n",
      "23\tValidation loss: 0.040758\tBest loss: 0.040758\tAccuracy: 98.55%\n",
      "24\tValidation loss: 0.040522\tBest loss: 0.040522\tAccuracy: 98.51%\n",
      "25\tValidation loss: 0.039474\tBest loss: 0.039474\tAccuracy: 98.63%\n",
      "26\tValidation loss: 0.038734\tBest loss: 0.038734\tAccuracy: 98.59%\n",
      "27\tValidation loss: 0.038390\tBest loss: 0.038390\tAccuracy: 98.59%\n",
      "28\tValidation loss: 0.037691\tBest loss: 0.037691\tAccuracy: 98.59%\n",
      "29\tValidation loss: 0.037356\tBest loss: 0.037356\tAccuracy: 98.55%\n",
      "30\tValidation loss: 0.036748\tBest loss: 0.036748\tAccuracy: 98.79%\n",
      "31\tValidation loss: 0.036298\tBest loss: 0.036298\tAccuracy: 98.75%\n",
      "32\tValidation loss: 0.035572\tBest loss: 0.035572\tAccuracy: 98.83%\n",
      "33\tValidation loss: 0.035834\tBest loss: 0.035572\tAccuracy: 98.71%\n",
      "34\tValidation loss: 0.035099\tBest loss: 0.035099\tAccuracy: 98.71%\n",
      "35\tValidation loss: 0.034824\tBest loss: 0.034824\tAccuracy: 98.71%\n",
      "36\tValidation loss: 0.035045\tBest loss: 0.034824\tAccuracy: 98.83%\n",
      "37\tValidation loss: 0.034397\tBest loss: 0.034397\tAccuracy: 98.79%\n",
      "38\tValidation loss: 0.034032\tBest loss: 0.034032\tAccuracy: 98.75%\n",
      "39\tValidation loss: 0.033624\tBest loss: 0.033624\tAccuracy: 98.79%\n",
      "40\tValidation loss: 0.033249\tBest loss: 0.033249\tAccuracy: 98.75%\n",
      "41\tValidation loss: 0.033322\tBest loss: 0.033249\tAccuracy: 98.75%\n",
      "42\tValidation loss: 0.032947\tBest loss: 0.032947\tAccuracy: 98.91%\n",
      "43\tValidation loss: 0.032586\tBest loss: 0.032586\tAccuracy: 98.83%\n",
      "44\tValidation loss: 0.032703\tBest loss: 0.032586\tAccuracy: 98.83%\n",
      "45\tValidation loss: 0.032459\tBest loss: 0.032459\tAccuracy: 98.87%\n",
      "46\tValidation loss: 0.032365\tBest loss: 0.032365\tAccuracy: 98.94%\n",
      "47\tValidation loss: 0.031913\tBest loss: 0.031913\tAccuracy: 98.91%\n",
      "48\tValidation loss: 0.031962\tBest loss: 0.031913\tAccuracy: 98.98%\n",
      "49\tValidation loss: 0.031929\tBest loss: 0.031913\tAccuracy: 98.87%\n",
      "50\tValidation loss: 0.031590\tBest loss: 0.031590\tAccuracy: 98.94%\n",
      "51\tValidation loss: 0.031381\tBest loss: 0.031381\tAccuracy: 98.91%\n",
      "52\tValidation loss: 0.031158\tBest loss: 0.031158\tAccuracy: 98.98%\n",
      "53\tValidation loss: 0.031136\tBest loss: 0.031136\tAccuracy: 98.91%\n",
      "54\tValidation loss: 0.030925\tBest loss: 0.030925\tAccuracy: 98.91%\n",
      "55\tValidation loss: 0.031131\tBest loss: 0.030925\tAccuracy: 98.91%\n",
      "56\tValidation loss: 0.030820\tBest loss: 0.030820\tAccuracy: 98.91%\n",
      "57\tValidation loss: 0.030919\tBest loss: 0.030820\tAccuracy: 98.91%\n",
      "58\tValidation loss: 0.030563\tBest loss: 0.030563\tAccuracy: 98.91%\n",
      "59\tValidation loss: 0.030468\tBest loss: 0.030468\tAccuracy: 98.94%\n",
      "60\tValidation loss: 0.030512\tBest loss: 0.030468\tAccuracy: 98.94%\n",
      "61\tValidation loss: 0.030369\tBest loss: 0.030369\tAccuracy: 98.94%\n",
      "62\tValidation loss: 0.030482\tBest loss: 0.030369\tAccuracy: 98.94%\n",
      "63\tValidation loss: 0.030029\tBest loss: 0.030029\tAccuracy: 98.98%\n",
      "64\tValidation loss: 0.030187\tBest loss: 0.030029\tAccuracy: 98.98%\n",
      "65\tValidation loss: 0.030250\tBest loss: 0.030029\tAccuracy: 98.91%\n",
      "66\tValidation loss: 0.030092\tBest loss: 0.030029\tAccuracy: 98.98%\n",
      "67\tValidation loss: 0.029999\tBest loss: 0.029999\tAccuracy: 98.94%\n",
      "68\tValidation loss: 0.030290\tBest loss: 0.029999\tAccuracy: 98.91%\n",
      "69\tValidation loss: 0.029974\tBest loss: 0.029974\tAccuracy: 98.94%\n",
      "70\tValidation loss: 0.030050\tBest loss: 0.029974\tAccuracy: 98.91%\n",
      "71\tValidation loss: 0.029718\tBest loss: 0.029718\tAccuracy: 98.91%\n",
      "72\tValidation loss: 0.029954\tBest loss: 0.029718\tAccuracy: 98.91%\n",
      "73\tValidation loss: 0.030068\tBest loss: 0.029718\tAccuracy: 98.91%\n",
      "74\tValidation loss: 0.029522\tBest loss: 0.029522\tAccuracy: 98.98%\n",
      "75\tValidation loss: 0.029875\tBest loss: 0.029522\tAccuracy: 98.94%\n",
      "76\tValidation loss: 0.029571\tBest loss: 0.029522\tAccuracy: 99.02%\n",
      "77\tValidation loss: 0.029577\tBest loss: 0.029522\tAccuracy: 98.98%\n",
      "78\tValidation loss: 0.029330\tBest loss: 0.029330\tAccuracy: 98.98%\n",
      "79\tValidation loss: 0.029569\tBest loss: 0.029330\tAccuracy: 98.98%\n",
      "80\tValidation loss: 0.029437\tBest loss: 0.029330\tAccuracy: 98.98%\n",
      "81\tValidation loss: 0.029516\tBest loss: 0.029330\tAccuracy: 98.94%\n",
      "82\tValidation loss: 0.029403\tBest loss: 0.029330\tAccuracy: 99.02%\n",
      "83\tValidation loss: 0.029656\tBest loss: 0.029330\tAccuracy: 98.91%\n",
      "84\tValidation loss: 0.029297\tBest loss: 0.029297\tAccuracy: 98.98%\n",
      "85\tValidation loss: 0.029435\tBest loss: 0.029297\tAccuracy: 98.98%\n",
      "86\tValidation loss: 0.029447\tBest loss: 0.029297\tAccuracy: 98.98%\n",
      "87\tValidation loss: 0.029238\tBest loss: 0.029238\tAccuracy: 98.98%\n",
      "88\tValidation loss: 0.029202\tBest loss: 0.029202\tAccuracy: 98.98%\n",
      "89\tValidation loss: 0.029321\tBest loss: 0.029202\tAccuracy: 98.98%\n",
      "90\tValidation loss: 0.029304\tBest loss: 0.029202\tAccuracy: 98.98%\n",
      "91\tValidation loss: 0.029319\tBest loss: 0.029202\tAccuracy: 99.02%\n",
      "92\tValidation loss: 0.029363\tBest loss: 0.029202\tAccuracy: 98.98%\n",
      "93\tValidation loss: 0.029192\tBest loss: 0.029192\tAccuracy: 98.98%\n",
      "94\tValidation loss: 0.029338\tBest loss: 0.029192\tAccuracy: 98.98%\n",
      "95\tValidation loss: 0.029281\tBest loss: 0.029192\tAccuracy: 98.98%\n",
      "96\tValidation loss: 0.029392\tBest loss: 0.029192\tAccuracy: 99.02%\n",
      "97\tValidation loss: 0.029400\tBest loss: 0.029192\tAccuracy: 98.98%\n",
      "98\tValidation loss: 0.029314\tBest loss: 0.029192\tAccuracy: 98.98%\n",
      "99\tValidation loss: 0.029208\tBest loss: 0.029192\tAccuracy: 99.02%\n",
      "100\tValidation loss: 0.029220\tBest loss: 0.029192\tAccuracy: 98.98%\n",
      "101\tValidation loss: 0.029418\tBest loss: 0.029192\tAccuracy: 99.02%\n",
      "102\tValidation loss: 0.029167\tBest loss: 0.029167\tAccuracy: 99.02%\n",
      "103\tValidation loss: 0.029368\tBest loss: 0.029167\tAccuracy: 99.02%\n",
      "104\tValidation loss: 0.029349\tBest loss: 0.029167\tAccuracy: 99.02%\n",
      "105\tValidation loss: 0.029260\tBest loss: 0.029167\tAccuracy: 99.02%\n",
      "106\tValidation loss: 0.029404\tBest loss: 0.029167\tAccuracy: 98.98%\n",
      "107\tValidation loss: 0.029294\tBest loss: 0.029167\tAccuracy: 99.02%\n",
      "108\tValidation loss: 0.029338\tBest loss: 0.029167\tAccuracy: 99.02%\n",
      "109\tValidation loss: 0.029254\tBest loss: 0.029167\tAccuracy: 99.06%\n",
      "110\tValidation loss: 0.029205\tBest loss: 0.029167\tAccuracy: 99.06%\n",
      "111\tValidation loss: 0.029184\tBest loss: 0.029167\tAccuracy: 99.06%\n",
      "112\tValidation loss: 0.029314\tBest loss: 0.029167\tAccuracy: 99.02%\n",
      "113\tValidation loss: 0.029351\tBest loss: 0.029167\tAccuracy: 99.10%\n",
      "114\tValidation loss: 0.029203\tBest loss: 0.029167\tAccuracy: 99.06%\n",
      "115\tValidation loss: 0.029223\tBest loss: 0.029167\tAccuracy: 99.06%\n",
      "116\tValidation loss: 0.029237\tBest loss: 0.029167\tAccuracy: 99.06%\n",
      "117\tValidation loss: 0.029244\tBest loss: 0.029167\tAccuracy: 99.06%\n",
      "118\tValidation loss: 0.029236\tBest loss: 0.029167\tAccuracy: 99.10%\n",
      "119\tValidation loss: 0.029288\tBest loss: 0.029167\tAccuracy: 99.06%\n",
      "120\tValidation loss: 0.029270\tBest loss: 0.029167\tAccuracy: 99.06%\n",
      "121\tValidation loss: 0.029457\tBest loss: 0.029167\tAccuracy: 99.06%\n",
      "122\tValidation loss: 0.029214\tBest loss: 0.029167\tAccuracy: 99.06%\n",
      "Early stopping!\n",
      "Final test accuracy: 99.34%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train1))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train1) // batch_size):\n",
    "            X_batch, y_batch = X_train1[rnd_indices], y_train1[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid1, y: y_valid1})\n",
    "        if loss_val < best_loss:\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "    acc_test = sess.run(accuracy, feed_dict={X: X_test1, y: y_test1})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\ell_1$ and $\\ell_2$ regularization\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next, we get a handle on the layer weights, and we compute the total loss, which is equal to the sum of the usual cross entropy loss and the $\\ell_1$ loss (i.e., the absolute values of the weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
    "\n",
    "scale = 0.001 # l1 regularization hyperparameter\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
    "    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.8292\n",
      "1 Test accuracy: 0.8654\n",
      "2 Test accuracy: 0.8792\n",
      "3 Test accuracy: 0.8887\n",
      "4 Test accuracy: 0.8946\n",
      "5 Test accuracy: 0.8979\n",
      "6 Test accuracy: 0.902\n",
      "7 Test accuracy: 0.9034\n",
      "8 Test accuracy: 0.9044\n",
      "9 Test accuracy: 0.9063\n",
      "10 Test accuracy: 0.9052\n",
      "11 Test accuracy: 0.907\n",
      "12 Test accuracy: 0.9072\n",
      "13 Test accuracy: 0.9083\n",
      "14 Test accuracy: 0.9075\n",
      "15 Test accuracy: 0.9064\n",
      "16 Test accuracy: 0.9064\n",
      "17 Test accuracy: 0.9058\n",
      "18 Test accuracy: 0.9062\n",
      "19 Test accuracy: 0.9055\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can pass a regularization function to the tf.layers.dense() function, which will use it to create operations that will compute the regularization loss, and it adds these operations to the collection of regularization losses. The beginning is the same as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use Python's partial() function to avoid repeating the same arguments over and over again. Note that we set the kernel_regularizer argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scale = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_dense_layer = partial(\n",
    "    tf.layers.dense, activation=tf.nn.relu,\n",
    "    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None,\n",
    "                            name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we must add the regularization losses to the base loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):                                     # not shown in the book\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(  # not shown\n",
    "        labels=y, logits=logits)                                # not shown\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")   # not shown\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.82\n",
      "1 Test accuracy: 0.8678\n",
      "2 Test accuracy: 0.8852\n",
      "3 Test accuracy: 0.8945\n",
      "4 Test accuracy: 0.9034\n",
      "5 Test accuracy: 0.9082\n",
      "6 Test accuracy: 0.9099\n",
      "7 Test accuracy: 0.9095\n",
      "8 Test accuracy: 0.9133\n",
      "9 Test accuracy: 0.9125\n",
      "10 Test accuracy: 0.9143\n",
      "11 Test accuracy: 0.915\n",
      "12 Test accuracy: 0.9156\n",
      "13 Test accuracy: 0.9168\n",
      "14 Test accuracy: 0.9166\n",
      "15 Test accuracy: 0.916\n",
      "16 Test accuracy: 0.9156\n",
      "17 Test accuracy: 0.9157\n",
      "18 Test accuracy: 0.9153\n",
      "19 Test accuracy: 0.916\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dropout\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the book uses tf.contrib.layers.dropout() rather than tf.layers.dropout() (which did not exist when this chapter was written). It is now preferable to use tf.layers.dropout(), because anything in the contrib module may change or be deleted without notice. The tf.layers.dropout() function is almost identical to the tf.contrib.layers.dropout() function, except for a few minor differences. Most importantly:\n",
    "\n",
    "- you must specify the dropout rate (rate) rather than the keep probability (keep_prob), where rate is simply equal to 1 - keep_prob,\n",
    "- the is_training parameter is renamed to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.924\n",
      "1 Test accuracy: 0.9363\n",
      "2 Test accuracy: 0.9497\n",
      "3 Test accuracy: 0.9552\n",
      "4 Test accuracy: 0.958\n",
      "5 Test accuracy: 0.9569\n",
      "6 Test accuracy: 0.9591\n",
      "7 Test accuracy: 0.9619\n",
      "8 Test accuracy: 0.9638\n",
      "9 Test accuracy: 0.9633\n",
      "10 Test accuracy: 0.9647\n",
      "11 Test accuracy: 0.9631\n",
      "12 Test accuracy: 0.9659\n",
      "13 Test accuracy: 0.9664\n",
      "14 Test accuracy: 0.9665\n",
      "15 Test accuracy: 0.9686\n",
      "16 Test accuracy: 0.9697\n",
      "17 Test accuracy: 0.9671\n",
      "18 Test accuracy: 0.9687\n",
      "19 Test accuracy: 0.9666\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", acc_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
