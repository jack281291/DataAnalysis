{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Avoiding Overfitting Through Regularization <center>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to implement this with TensorFlow is to evaluate the model on a validation\n",
    "set at regular intervals (e.g., every 50 steps), and save a “winner” snapshot if it outperforms previous “winner” snapshots. Count the number of steps since the last “win‐\n",
    "ner” snapshot was saved, and interrupt training when this number reaches some limit\n",
    "(e.g., 2,000 steps). Then restore the last “winner” snapshot.\n",
    "Although early stopping works very well in practice, you can usually get much higher\n",
    "performance out of your network by combining it with other regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ℓ1 and ℓ2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to do this using TensorFlow is to simply add the appropriate regularization\n",
    "terms to your cost function. For example, assuming you have just one hidden layer\n",
    "with weights W1 and one output layer with weights W2, then you can apply ℓ1 regularization like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[...] # construct the neural network\n",
    "W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
    "scale = 0.001 # l1 regularization hyperparameter\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                            logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
    "    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if there are many layers, this approach is not very convenient. Fortunately,\n",
    "TensorFlow provides a better option. Many functions that create variables (such as\n",
    "get_variable() or tf.layers.dense()) accept a *_regularizer argument for each\n",
    "created variable (e.g., kernel_regularizer). You can pass any function that takes\n",
    "weights as an argument and returns the corresponding regularization loss. The\n",
    "l1_regularizer(), l2_regularizer(), and l1_l2_regularizer() functions return\n",
    "such functions. The following code puts all this together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_dense_layer = partial(tf.layers.dense, activation=tf.nn.relu,\n",
    "                        kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None,\n",
    "                            name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a neural network with two hidden layers and one output layer, and\n",
    "it also creates nodes in the graph to compute the ℓ1 regularization loss corresponding\n",
    "to each layer’s weights. TensorFlow automatically adds these nodes to a special collection containing all the regularization losses. You just need to add these regularization\n",
    "losses to your overall loss, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vars = tf.trainable_variables() \n",
    "lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in vars\n",
    "                    if 'bias' not in v.name ]) * 0.001\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y, logits=logits)\n",
    "    loss = tf.reduce_mean(cross_entropy + lossL2, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most popular regularization technique for deep neural networks is arguably\n",
    "dropout. It was proposed21 by G. E. Hinton in 2012 and further detailed in a paper by\n",
    "Nitish Srivastava et al., and it has proven to be highly successful: even the state-ofthe-art neural networks got a 1–2% accuracy boost simply by adding dropout. This\n",
    "may not sound like a lot, but when a model already has 95% accuracy, getting a 2%\n",
    "accuracy boost means dropping the error rate by almost 40% (going from 5% error to\n",
    "roughly 3%).\n",
    "It is a fairly simple algorithm: at every training step, every neuron (including the\n",
    "input neurons but excluding the output neurons) has a probability p of being temporarily “dropped out,” meaning it will be entirely ignored during this training step,\n",
    "but it may be active during the next step. The hyperparameter p is\n",
    "called the dropout rate, and it is typically set to 50%. After training, neurons don’t get\n",
    "dropped anymore. And that’s all (except for a technical detail we will discuss momentarily). It is quite surprising at first that this rather brutal technique works at all. Would a\n",
    "company perform better if its employees were told to toss a coin every morning to\n",
    "decide whether or not to go to work? Well, who knows; perhaps it would! The com‐\n",
    "pany would obviously be forced to adapt its organization; it could not rely on any sin‐\n",
    "gle person to fill in the coffee machine or perform any other critical tasks, so this\n",
    "expertise would have to be spread across several people. Employees would have to\n",
    "learn to cooperate with many of their coworkers, not just a handful of them. The company would become much more resilient. If one person quit, it wouldn’t make\n",
    "much of a difference. It’s unclear whether this idea would actually work for companies, but it certainly does for neural networks. Neurons trained with dropout cannot\n",
    "co-adapt with their neighboring neurons; they have to be as useful as possible on\n",
    "their own. They also cannot rely excessively on just a few input neurons; they must\n",
    "pay attention to each of their input neurons. They end up being less sensitive to slight\n",
    "changes in the inputs. In the end you get a more robust network that generalizes better.\n",
    "\n",
    "Another way to understand the power of dropout is to realize that a unique neural\n",
    "network is generated at each training step. Since each neuron can be either present or\n",
    "absent, there is a total of 2N possible networks (where N is the total number of drop‐\n",
    "pable neurons). This is such a huge number that it is virtually impossible for the same\n",
    "neural network to be sampled twice. Once you have run a 10,000 training steps, you\n",
    "have essentially trained 10,000 different neural networks (each with just one training\n",
    "instance). These neural networks are obviously not independent since they share\n",
    "many of their weights, but they are nevertheless all different. The resulting neural\n",
    "network can be seen as an averaging ensemble of all these smaller neural networks.\n",
    "There is one small but important technical detail. **Suppose p = 50%, in which case\n",
    "during testing a neuron will be connected to twice as many input neurons as it was\n",
    "(on average) during training. To compensate for this fact, we need to multiply each\n",
    "neuron’s input connection weights by 0.5 after training. If we don’t, each neuron will\n",
    "get a total input signal roughly twice as large as what the network was trained on, and\n",
    "it is unlikely to perform well. More generally, we need to multiply each input connection weight by the keep probability (1 – p) after training. Alternatively, we can divide\n",
    "each neuron’s output by the keep probability during training (these alternatives are\n",
    "not perfectly equivalent, but they work equally well).\n",
    "To implement dropout using TensorFlow, you can simply apply the tf.layers.dropout() function to the input layer and/or to the output of any hidden layer you want.\n",
    "During training, this function randomly drops some items (setting them to 0) and\n",
    "divides the remaining items by the keep probability. After training, this function does\n",
    "nothing at all. The following code applies dropout regularization to our three-layer\n",
    "neural network:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[...]\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "dropout_rate = 0.5 # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
    "                                name=\"hidden1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu,\n",
    "                                name=\"hidden2\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.s. You want to use the tf.layers.dropout() function, not\n",
    "tf.nn.dropout(). The first one turns off (no-op) when not training, which is what you want, while the second one does not.\n",
    "\n",
    "**Of course, just like you did earlier for Batch Normalization, you need to set training\n",
    "to True when training, and leave the default False value when testing.\n",
    "If you observe that the model is overfitting, you can increase the dropout rate. Conversely, you should try decreasing the dropout rate if the model underfits the training\n",
    "set. It can also help to increase the dropout rate for large layers, and reduce it for\n",
    "small ones.\n",
    "Dropout does tend to significantly slow down convergence, but it usually results in a\n",
    "much better model when tuned properly. So, it is generally well worth the extra time\n",
    "and effort.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max-Norm Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another regularization technique that is quite popular for neural networks is called\n",
    "max-norm regularization: for each neuron, it constrains the weights w of the incoming connections such that $∥ w ∥_2$ ≤ r, where r is the max-norm hyperparameter and\n",
    "$∥ · ∥_2$ is the ℓ2 norm.\n",
    "We typically implement this constraint by computing $∥w∥_2$ after each training step\n",
    "and clipping w if needed (w <- w*r/ $∥ w ∥_2$).\n",
    "Reducing r increases the amount of regularization and helps reduce overfitting. Maxnorm regularization can also help alleviate the vanishing/exploding gradients problems (if you are not using Batch Normalization).\n",
    "TensorFlow does not provide an off-the-shelf max-norm regularizer, but it is not too\n",
    "hard to implement. The following code gets a handle on the weights of the first hidden layer, then it uses the clip_by_norm() function to create an operation that will clip the weights along the second axis so that each row vector ends up with a maximum norm of 1.0. The last line creates an assignment operation that will assign the\n",
    "clipped weights to the weights variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "weights = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "clipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\n",
    "clip_weights = tf.assign(weights, clipped_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you just apply this operation after each training step, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "clip_weights.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, you would do this for every hidden layer. Although this solution should\n",
    "work fine, it is a bit messy. A cleaner solution is to create a max_norm_regularizer()\n",
    "function and use it just like the earlier l1_regularizer() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\",\n",
    "                        collection=\"max_norm\"):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name=name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None # there is no regularization loss term\n",
    "    return max_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns a parametrized max_norm() function that you can use like any\n",
    "other regularizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_norm_reg = max_norm_regularizer(threshold=1.0)\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "    kernel_regularizer=max_norm_reg, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "    kernel_regularizer=max_norm_reg, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that max-norm regularization does not require adding a regularization loss term\n",
    "to your overall loss function, which is why the max_norm() function returns None. But\n",
    "you still need to be able to run the clip_weights operations after each training step,\n",
    "so you need to be able to get a handle on them. This is why the max_norm() function\n",
    "adds the clip_weights operation to a collection of max-norm clipping operations.\n",
    "You need to fetch these clipping operations and run them after each training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clip_all_weights = tf.get_collection(\"max_norm\")\n",
    "    with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            sess.run(clip_all_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last regularization technique, data augmentation, consists of generating new\n",
    "training instances from existing ones, artificially boosting the size of the training set.\n",
    "This will reduce overfitting, making this a regularization technique. The trick is to\n",
    "generate realistic training instances; ideally, a human should not be able to tell which\n",
    "instances were generated and which ones were not. Moreover, simply adding white\n",
    "noise will not help; the modifications you apply should be learnable (white noise is\n",
    "not).\n",
    "For example, if your model is meant to classify pictures of mushrooms, you can\n",
    "slightly shift, rotate, and resize every picture in the training set by various amounts\n",
    "and add the resulting pictures to the training set (see Figure 11-10). This forces the\n",
    "model to be more tolerant to the position, orientation, and size of the mushrooms in\n",
    "the picture. If you want the model to be more tolerant to lighting conditions, you can\n",
    "similarly generate many images with various contrasts. Assuming the mushrooms are\n",
    "symmetrical, you can also flip the pictures horizontally. By combining these transformations you can greatly increase the size of your training set. It is often preferable to generate training instances on the fly during training rather\n",
    "than wasting storage space and network bandwidth. TensorFlow offers several image\n",
    "manipulation operations such as transposing (shifting), rotating, resizing, flipping,\n",
    "and cropping, as well as adjusting the brightness, contrast, saturation, and hue (see\n",
    "the API documentation for more details). This makes it easy to implement data augmentation for image datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
