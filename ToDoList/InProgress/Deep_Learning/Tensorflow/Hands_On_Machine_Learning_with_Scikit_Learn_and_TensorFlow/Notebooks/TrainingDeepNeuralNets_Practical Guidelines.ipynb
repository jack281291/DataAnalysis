{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Practical Guidelines <center>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration in Table 11-2 will work fine in\n",
    "most cases.\n",
    "\n",
    "\n",
    "Table 11-2. \n",
    "**Default DNN configuration**\n",
    "- Initialization  --  He initialization\n",
    "- Activation function  --  ELU\n",
    "- Normalization  --   Batch Normalization\n",
    "- Regularization  --  Dropout\n",
    "- Optimizer  --  Nesterov Accelerated Gradient\n",
    "- Learning rate schedule  --  None\n",
    "\n",
    "\n",
    "Of course, you should try to reuse parts of a pretrained neural network if you can\n",
    "find one that solves a similar problem.\n",
    "This default configuration may need to be tweaked:\n",
    "- If you can’t find a good learning rate (convergence was too slow, so you increased the training rate, and now convergence is fast but the network’s accuracy is suboptimal), then you can try adding a learning schedule such as exponential decay.\n",
    "- If your training set is a bit too small, you can implement data augmentation.\n",
    "- If you need a sparse model, you can add some ℓ1 regularization to the mix (and optionally zero out the tiny weights after training). If you need an even sparser model, you can try using FTRL instead of Adam optimization, along with ℓ1 regularization.\n",
    "- If you need a lightning-fast model at runtime, you may want to drop Batch Normalization, and possibly replace the ELU activation function with the leaky ReLU. Having a sparse model will also help.\n",
    "\n",
    "\n",
    "With these guidelines, you are now ready to train very deep nets—well, if you are\n",
    "very patient, that is! If you use a single machine, you may have to wait for days or\n",
    "even months for training to complete. In the next chapter we will discuss how to use\n",
    "distributed TensorFlow to train and run models across many servers and GPUs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
