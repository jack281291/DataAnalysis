{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import theano\n",
    "import tensorflow\n",
    "from keras.datasets import mnist # subroutines for fetching the MNIST dataset\n",
    "from keras.models import Model # basic class for specifying and training a neural network\n",
    "from keras.layers import Input, Dense # the two types of neural network layer we will be using\n",
    "from keras.utils import np_utils # utilities for one-hot encoding of ground truth values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we’ll define some parameters of our model. These are often called hyperparameters, because they are assumed to be fixed before training starts. For the purposes of this tutorial, we will stick to using some sensible values, but keep in mind that properly training them is a significant issue, which will be addressed more properly in a future tutorial.\n",
    "\n",
    "In particular, we will define:\n",
    "- The batch size, representing the number of training examples being used simultaneously during a single iteration of the gradient descent algorithm;\n",
    "- The number of epochs, representing the number of times the training algorithm will iterate over the entire training set before terminating;\n",
    "- The number of neurons in each of the two hidden layers of the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128 # in each iteration, we consider 128 training examples at once\n",
    "num_epochs = 20 # we iterate twenty times over the entire training set\n",
    "hidden_size = 512 # there will be 512 neurons in both hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to load and preprocess the MNIST data set. Keras makes this extremely simple, with a fixed interface for fetching and extracting the data from the remote server, directly into NumPy arrays.\n",
    "\n",
    "To preprocess the input data, we will first flatten the images into 1D (as we will consider each pixel as a separate input feature), and we will then force the pixel intensity values to be in the [0,1] range by dividing them by 255. This is a very simple way to “normalise” the data, and I will be discussing other ways in future tutorials in this series.\n",
    "\n",
    "A good approach to a classification problem is to use probabilistic classification, i.e. to have a single output neuron for each class, outputting a value which corresponds to the probability of the input being of that particular class. This implies a need to transform the training output data into a “one-hot” encoding: for example, if the desired output class is 3, and there are five classes overall (labelled 0 to 4), then an appropriate one-hot encoding is: [0 0 0 1 0]. Keras, once again, provides us with an out-of-the-box functionality for doing just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.pkl.gz\n",
      "15269888/15296311 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "num_train = 60000 # there are 60000 training examples in MNIST\n",
    "num_test = 10000 # there are 10000 test examples in MNIST\n",
    "\n",
    "height, width, depth = 28, 28, 1 # MNIST images are 28x28 and greyscale\n",
    "num_classes = 10 # there are 10 classes (1 per digit)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() # fetch MNIST data\n",
    "\n",
    "X_train = X_train.reshape(num_train, height * width) # Flatten data to 1D\n",
    "X_test = X_test.reshape(num_test, height * width) # Flatten data to 1D\n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255 # Normalise data to [0, 1] range\n",
    "X_test /= 255 # Normalise data to [0, 1] range\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, num_classes) # One-hot encode the labels\n",
    "Y_test = np_utils.to_categorical(y_test, num_classes) # One-hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[59999][783]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the time to actually define our model! To do this we will be using a stack of three Dense layers, which correspond to a fully unrestricted MLP structure, linking all of the outputs of the previous layer to the inputs of the next one. We will use ReLU activations for the neurons in the first two layers, and a softmax activation for the neurons in the final one. This activation is designed to turn any real-valued vector into a vector of probabilities, and is defined as follows, for the j-th neuron:\n",
    "\n",
    "σ(z)_j=exp⁡(zj)∑iexp⁡(zi)\n",
    "\n",
    "An excellent feature of Keras, that sets it apart from frameworks such as TensorFlow, is automatic inference of shapes; we only need to specify the shape of the input layer, and afterwards Keras will take care of initialising the weight variables with proper shapes. Once all the layers have been defined, we simply need to identify the input(s) and the output(s) in order to define our model, as illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(height * width,)) # Our input is a 1D vector of size 784\n",
    "hidden_1 = Dense(hidden_size, activation='relu')(inp) # First hidden ReLU layer\n",
    "hidden_2 = Dense(hidden_size, activation='relu')(hidden_1) # Second hidden ReLU layer\n",
    "out = Dense(num_classes, activation='softmax')(hidden_2) # Output softmax layer\n",
    "\n",
    "model = Model(input=inp, output=out) # To define a model, just specify its input and output layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish off specifying the model, we need to define our loss function, the optimisation algorithm to use, and which metrics to report.\n",
    "\n",
    "When dealing with probabilistic classification, it is actually better to use the cross-entropy loss, rather than the previously defined squared error. For a particular output probability vector y, compared with our (ground truth) one-hot vector y^, the loss (for k-class classification) is defined by:\n",
    "\n",
    "L(y,y^)=−∑y^_i * ln(⁡y_i)\n",
    "\n",
    "This loss is better for probabilistic tasks (i.e. ones with logistic/softmax output neurons), primarily because of its manner of derivation—it aims only to maximise the model’s confidence in the correct class, and is not concerned with the distribution of probabilities for other classes (while the squared error loss would dedicate equal attention to getting all of the other class probabilities as close to zero as possible). This is due to the fact that incorrect classes, i.e. classes i′ with y^i′=0, eliminate the respective neuron’s output from the loss function.\n",
    "\n",
    "The optimisation algorithm used will typically revolve around some form of gradient descent; their key differences revolve around the manner in which the previously mentioned learning rate η is chosen or adapted during training. An excellent overview of such approaches is given by this blog post; here we will use the Adam optimiser, which typically performs well.\n",
    "\n",
    "As our classes are balanced (there is an equal amount of handwritten digits across all ten classes), an appropriate metric to report is the accuracy; the proportion of the inputs classified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function\n",
    "              optimizer='adam', # using the Adam optimiser\n",
    "              metrics=['accuracy']) # reporting the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we call the training algorithm with the determined batch size and epoch count. It is good practice to set aside a fraction of the training data to be used just for verification that our algorithm is (still) properly generalising (this is commonly referred to as the validation set); here we will hold out 10% of the data for this purpose.\n",
    "\n",
    "An excellent out-of-the-box feature of Keras is verbosity; it’s able to provide detailed real-time pretty-printing of the training algorithm’s progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/20\n",
      "54000/54000 [==============================] - 14s - loss: 0.2324 - acc: 0.9321 - val_loss: 0.0926 - val_acc: 0.9715\n",
      "Epoch 2/20\n",
      "54000/54000 [==============================] - 14s - loss: 0.0838 - acc: 0.9737 - val_loss: 0.0695 - val_acc: 0.9777\n",
      "Epoch 3/20\n",
      "54000/54000 [==============================] - 15s - loss: 0.0553 - acc: 0.9833 - val_loss: 0.0668 - val_acc: 0.9793\n",
      "Epoch 4/20\n",
      "54000/54000 [==============================] - 13s - loss: 0.0373 - acc: 0.9879 - val_loss: 0.0667 - val_acc: 0.9807\n",
      "Epoch 5/20\n",
      "54000/54000 [==============================] - 13s - loss: 0.0288 - acc: 0.9906 - val_loss: 0.0646 - val_acc: 0.9820\n",
      "Epoch 6/20\n",
      "54000/54000 [==============================] - 13s - loss: 0.0226 - acc: 0.9923 - val_loss: 0.0794 - val_acc: 0.9797\n",
      "Epoch 7/20\n",
      "54000/54000 [==============================] - 14s - loss: 0.0175 - acc: 0.9941 - val_loss: 0.0748 - val_acc: 0.9805\n",
      "Epoch 8/20\n",
      "54000/54000 [==============================] - 15s - loss: 0.0165 - acc: 0.9942 - val_loss: 0.0714 - val_acc: 0.9817\n",
      "Epoch 9/20\n",
      "54000/54000 [==============================] - 14s - loss: 0.0163 - acc: 0.9948 - val_loss: 0.0754 - val_acc: 0.9795\n",
      "Epoch 10/20\n",
      "54000/54000 [==============================] - 14s - loss: 0.0138 - acc: 0.9952 - val_loss: 0.0790 - val_acc: 0.9815\n",
      "Epoch 11/20\n",
      "54000/54000 [==============================] - 13s - loss: 0.0115 - acc: 0.9964 - val_loss: 0.0797 - val_acc: 0.9827\n",
      "Epoch 12/20\n",
      "54000/54000 [==============================] - 15s - loss: 0.0104 - acc: 0.9964 - val_loss: 0.1066 - val_acc: 0.9777\n",
      "Epoch 13/20\n",
      "54000/54000 [==============================] - 20s - loss: 0.0123 - acc: 0.9957 - val_loss: 0.0822 - val_acc: 0.9817\n",
      "Epoch 14/20\n",
      "54000/54000 [==============================] - 16s - loss: 0.0105 - acc: 0.9969 - val_loss: 0.0763 - val_acc: 0.9828\n",
      "Epoch 15/20\n",
      "54000/54000 [==============================] - 15s - loss: 0.0090 - acc: 0.9971 - val_loss: 0.0853 - val_acc: 0.9827\n",
      "Epoch 16/20\n",
      "54000/54000 [==============================] - 14s - loss: 0.0072 - acc: 0.9976 - val_loss: 0.1093 - val_acc: 0.9788\n",
      "Epoch 17/20\n",
      "54000/54000 [==============================] - 16s - loss: 0.0131 - acc: 0.9959 - val_loss: 0.0858 - val_acc: 0.9828\n",
      "Epoch 18/20\n",
      "54000/54000 [==============================] - 15s - loss: 0.0064 - acc: 0.9977 - val_loss: 0.0885 - val_acc: 0.9828\n",
      "Epoch 19/20\n",
      "54000/54000 [==============================] - 16s - loss: 0.0094 - acc: 0.9971 - val_loss: 0.1038 - val_acc: 0.9803\n",
      "Epoch 20/20\n",
      "54000/54000 [==============================] - 17s - loss: 0.0067 - acc: 0.9980 - val_loss: 0.0895 - val_acc: 0.9813\n",
      "10000/10000 [==============================] - 2s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.090043347067890772, 0.98219999999999996]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, # Train the model using the training set...\n",
    "          batch_size=batch_size, nb_epoch=num_epochs,\n",
    "          verbose=1, validation_split=0.1) # ...holding out 10% of the data for validation\n",
    "model.evaluate(X_test, Y_test, verbose=1) # Evaluate the trained model on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
