{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating ensembles from submission files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic and convenient way to ensemble is to ensemble submission CSV files. You only need the predictions on the test set for these methods — no need to retrain a model. This makes it a quick way to ensemble already existing model predictions, ideal when teaming up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting ensembles\n",
    "\n",
    "Model ensembling reduces error rate and it works better to ensemble low-correlated model predictions. We simply create a new classifier that predict a label when the majority of the classifiers vote for that label.\n",
    "Majority votes make most sense when the evaluation metric requires hard predictions, for instance with (multiclass-) classification accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighting\n",
    "\n",
    "We then use a weighted majority vote. Why weighing? Usually we want to give a better model more weight in a vote. We can expect this ensemble to repair a few erroneous choices by the best model, leading to a small improvement only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging\n",
    "\n",
    "Averaging works well for a wide range of problems (both classification and regression) and metrics (AUC, squared error or logaritmic loss).\n",
    "\n",
    "There is not much more to averaging than taking the mean of individual model predictions. An often heard shorthand for this on Kaggle is “bagging submissions”.\n",
    "\n",
    "Averaging predictions often reduces overfit. You ideally want a smooth separation between classes, and a single model’s predictions can be a little rough around the edges.\n",
    "\n",
    "**Geometric mean can outperform a plain average.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank averaging\n",
    "\n",
    "When averaging the outputs from multiple different models some problems can pop up. Not all predictors are perfectly calibrated: they may be over- or underconfident when predicting a low or high probability. Or the predictions clutter around a certain range.\n",
    "\n",
    "In the extreme case you may have a submission which looks like this:\n",
    "\n",
    "**Id,Prediction\n",
    "1,0.35000056\n",
    "2,0.35000002\n",
    "3,0.35000098\n",
    "4,0.35000111**\n",
    "\n",
    "Such a prediction may do well on the leaderboard when the evaluation metric is ranking or threshold based like AUC. But when averaged with another model like:\n",
    "\n",
    "**Id,Prediction\n",
    "1,0.57\n",
    "2,0.04\n",
    "3,0.96\n",
    "4,0.99**\n",
    "\n",
    "it will not change the ensemble much at all.\n",
    "\n",
    "Our solution is to first turn the predictions into ranks, then averaging these ranks.\n",
    "\n",
    "**Id,Rank,Prediction\n",
    "1,1,0.35000056\n",
    "2,0,0.35000002\n",
    "3,2,0.35000098\n",
    "4,3,0.35000111**\n",
    "\n",
    "After normalizing the averaged ranks between 0 and 1 you are sure to get an even distribution in your predictions. The resulting rank-averaged ensemble:\n",
    "\n",
    "**Id,Prediction\n",
    "1,0.33\n",
    "2,0.0\n",
    "3,0.66\n",
    "4,1.0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Generalization & Blending\n",
    "\n",
    "Averaging prediction files is nice and easy, but it’s not the only method that the top Kagglers are using. The serious gains start with stacking and blending. Hold on to your top-hats and petticoats: Here be dragons. With 7 heads. Standing on top of 30 other dragons.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked generalization\n",
    "\n",
    "Stacked generalization was introduced by Wolpert in a 1992 paper, 2 years before the seminal Breiman paper “Bagging Predictors“. Wolpert is famous for another very popular machine learning theorem: “There is no free lunch in search and optimization“.\n",
    "\n",
    "The basic idea behind stacked generalization is to use a pool of base classifiers, then using another classifier to combine their predictions, with the aim of reducing the generalization error.\n",
    "\n",
    "Let’s say you want to do 2-fold stacking:\n",
    "\n",
    "Split the train set in 2 parts: train_a and train_b\n",
    "Fit a first-stage model on train_a and create predictions for train_b\n",
    "Fit the same model on train_b and create predictions for train_a\n",
    "Finally fit the model on the entire train set and create predictions for the test set.\n",
    "Now train a second-stage stacker model on the probabilities from the first-stage model(s).\n",
    "A stacker model gets more information on the problem space by using the first-stage predictions as features, than if it was trained in isolation.\n",
    "\n",
    "#### Blending\n",
    "\n",
    "Blending is a word introduced by the Netflix winners. It is very close to stacked generalization, but a bit simpler and less risk of an information leak. Some researchers use “stacked ensembling” and “blending” interchangeably.\n",
    "\n",
    "With blending, instead of creating out-of-fold predictions for the train set, you create a small holdout set of say 10% of the train set. The stacker model then trains on this holdout set only.\n",
    "\n",
    "Blending has a few benefits:\n",
    "\n",
    "It is simpler than stacking.\n",
    "It wards against an information leak: The generalizers and stackers use different data.\n",
    "You do not need to share a seed for stratified folds with your teammates. Anyone can throw models in the ‘blender’ and the blender decides if it wants to keep that model or not.\n",
    "The cons are:\n",
    "\n",
    "You use less data overall\n",
    "The final model may overfit to the holdout set.\n",
    "Your CV is more solid with stacking (calculated over more folds) than using a single small holdout set.\n",
    "As for performance, both techniques are able to give similar results, and it seems to be a matter of preference and skill which you prefer. I myself prefer stacking.\n",
    "\n",
    "If you can not choose, you can always do both. Create stacked ensembles with stacked generalization and out-of-fold predictions. Then use a holdout set to further combine these models at a third stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======================================================================================================\n",
    "Summary:\n",
    "Just to test an implementation of stacking. Using a cross-validated random forest and SVMs, I was\n",
    "only able to achieve an accuracy of about 88% (with 1000 trees and up). Using stacked generalization \n",
    "I have seen a maximum of 93.5% accuracy. It does take runs to find it out though. This uses only \n",
    "(10, 20, 10) trees for the three classifiers.\n",
    "This code is heavily inspired from the code shared by Emanuele (https://github.com/emanuele) , but I\n",
    "have cleaned it up to makeit available for easy download and execution.\n",
    "======================================================================================================\n",
    "Methodology:\n",
    "Three classifiers (RandomForestClassifier, ExtraTreesClassifier and a GradientBoostingClassifier\n",
    "are built to be stacked by a LogisticRegression in the end.\n",
    "Some terminologies first, since everyone has their own, I'll define mine to be clear:\n",
    "- DEV SET, this is to be split into the training and validation data. It will be cross-validated.\n",
    "- TEST SET, this is the unseen data to validate the generalization error of our final classifier. This\n",
    "set will never be used to train.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import load_data\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def logloss(attempt, actual, epsilon=1.0e-15):\n",
    "    \"\"\"Logloss, i.e. the score of the bioresponse competition.\n",
    "    \"\"\"\n",
    "    attempt = np.clip(attempt, epsilon, 1.0-epsilon)\n",
    "    return - np.mean(actual * np.log(attempt) +\n",
    "                     (1.0 - actual) * np.log(1.0 - attempt))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    np.random.seed(0)  # seed to shuffle the train set\n",
    "\n",
    "    n_folds = 10\n",
    "    verbose = True\n",
    "    shuffle = False\n",
    "\n",
    "    X, y, X_submission = load_data.load()\n",
    "\n",
    "    if shuffle:\n",
    "        idx = np.random.permutation(y.size)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    skf = list(StratifiedKFold(y, n_folds))\n",
    "\n",
    "    clfs = [RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "            RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "            ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "            ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "            GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=50)]\n",
    "\n",
    "    print(\"Creating train and test sets for blending.\")\n",
    "\n",
    "    dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n",
    "    dataset_blend_test = np.zeros((X_submission.shape[0], len(clfs)))\n",
    "\n",
    "    for j, clf in enumerate(clfs):\n",
    "        print(j, clf)\n",
    "        dataset_blend_test_j = np.zeros((X_submission.shape[0], len(skf)))\n",
    "        for i, (train, test) in enumerate(skf):\n",
    "            print(\"Fold\", i)\n",
    "            X_train = X[train]\n",
    "            y_train = y[train]\n",
    "            X_test = X[test]\n",
    "            y_test = y[test]\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_submission = clf.predict_proba(X_test)[:, 1]\n",
    "            dataset_blend_train[test, j] = y_submission\n",
    "            dataset_blend_test_j[:, i] = clf.predict_proba(X_submission)[:, 1]\n",
    "        dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)\n",
    "\n",
    "    print(\"Blending.\")\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(dataset_blend_train, y)\n",
    "    y_submission = clf.predict_proba(dataset_blend_test)[:, 1]\n",
    "\n",
    "    print(\"Linear stretch of predictions to [0,1]\")\n",
    "    y_submission = (y_submission - y_submission.min()) / (y_submission.max() - y_submission.min())\n",
    "\n",
    "    print(\"Saving Results.\")\n",
    "    tmp = np.vstack([range(1, len(y_submission)+1), y_submission]).T\n",
    "    np.savetxt(fname='submission.csv', X=tmp, fmt='%d,%0.9f',\n",
    "               header='MoleculeId,PredictedProbability', comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous code we use all the trained folder model to make a prediction on the test set and then take the mean, you can also train the same model on the entire train set and then manke the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking classifiers with regressors and vice versa\n",
    "\n",
    "Stacking allows you to use classifiers for regression problems and vice versa. For instance, one may try a base model with quantile regression on a binary classification problem. A good stacker should be able to take information from the predictions, even though usually regression is not the best classifier.\n",
    "\n",
    "Using classifiers for regression problems is a bit trickier. You use binning first: You turn the y-label into evenly spaced classes. A regression problem that requires you to predict wages can be turned into a multiclass classification problem like so:\n",
    "\n",
    "Everything under 20k is class 1.\n",
    "Everything between 20k and 40k is class 2.\n",
    "Everything over 40k is class 3.\n",
    "The predicted probabilities for these classes can help a stacking regressor make better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everything is a hyper-parameter\n",
    "\n",
    "When doing stacking/blending/meta-modeling it is healthy to think of every action as a hyper-parameter for the stacker model.\n",
    "\n",
    "So for instance:\n",
    "\n",
    "Not scaling the data\n",
    "Standard-Scaling the data\n",
    "Minmax scaling the data\n",
    "are simply extra parameters to be tuned to improve the ensemble performance. Likewise, the number of base models to use can be seen as a parameter to optimize. Feature selection (top 70%) or imputation (impute missing features with a 0) are other examples of meta-parameters.\n",
    "\n",
    "Like a random gridsearch is a good candidate for tuning algorithm parameters, so does it work for tuning these meta-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
