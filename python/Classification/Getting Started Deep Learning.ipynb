{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/resources'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/resources/data/Mnist\n"
     ]
    }
   ],
   "source": [
    "cd data/Mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle, gzip, numpy\n",
    "\n",
    "# Load the dataset\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = cPickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shared_dataset(data_xy):\n",
    "    \"\"\" Function that loads the dataset into shared variables\n",
    "\n",
    "    The reason we store our dataset in shared variables is to allow\n",
    "    Theano to copy it into the GPU memory (when code is run on GPU).\n",
    "    Since copying data into the GPU is slow, copying a minibatch everytime\n",
    "    is needed (the default behaviour if the data is not in a shared\n",
    "    variable) would lead to a large decrease in performance.\n",
    "    \"\"\"\n",
    "    data_x, data_y = data_xy\n",
    "    shared_x = theano.shared(numpy.asarray(data_x, dtype=theano.config.floatX))\n",
    "    shared_y = theano.shared(numpy.asarray(data_y, dtype=theano.config.floatX))\n",
    "    # When storing data on the GPU it has to be stored as floats\n",
    "    # therefore we will store the labels as ``floatX`` as well\n",
    "    # (``shared_y`` does exactly that). But during our computations\n",
    "    # we need them as ints (we use labels as index, and if they are\n",
    "    # floats it doesn't make sense) therefore instead of returning\n",
    "    # ``shared_y`` we will have to cast it to int. This little hack\n",
    "    # lets us get around this issue\n",
    "    return shared_x, T.cast(shared_y, 'int32')\n",
    "\n",
    "test_set_x, test_set_y = shared_dataset(test_set)\n",
    "valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
    "train_set_x, train_set_y = shared_dataset(train_set)\n",
    "\n",
    "batch_size = 500    # size of the minibatch\n",
    "\n",
    "# accessing the third minibatch of the training set\n",
    "\n",
    "data  = train_set_x[2 * batch_size: 3 * batch_size]\n",
    "label = train_set_y[2 * batch_size: 3 * batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions to optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Log-Likelihood Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NLL is a symbolic variable ; to get the actual value of NLL, this symbolic\n",
    "# expression has to be compiled into a Theano function (see the Theano\n",
    "# tutorial for more details)\n",
    "def NLL(p_y_given_x, y):\n",
    "    return -T.sum(T.log(p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "# note on syntax: T.arange(y.shape[0]) is a vector of integers [0,1,2,...,len(y)].\n",
    "# Indexing a matrix M by the two vectors [0,1,...,K], [a,b,...,k] returns the\n",
    "# elements M[0,a], M[1,b], ..., M[K,k] as a vector.  Here, we use this\n",
    "# syntax to retrieve the log-probability of the correct labels, y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is ordinary gradient descent? it is a simple algorithm in which we repeatedly make small steps downward on an error surface defined by a loss function of some parameters. For the purpose of ordinary gradient descent we consider that the training data is rolled into the loss function. Then the pseudocode of this algorithm can be described as :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADIENT DESCENT\n",
    "\n",
    "while True:\n",
    "    loss = f(params)\n",
    "    d_loss_wrt_params = ... # compute gradient\n",
    "    params -= learning_rate * d_loss_wrt_params\n",
    "    if <stopping condition is met>:\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent (SGD) works according to the same principles as ordinary gradient descent, but proceeds more quickly by estimating the gradient from just a few examples at a time instead of the entire training set. In its purest form, we estimate the gradient from just a single example at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STOCHASTIC GRADIENT DESCENT\n",
    "for (x_i,y_i) in training_set:\n",
    "                            # imagine an infinite generator\n",
    "                            # that may repeat examples (if there is only a finite training set)\n",
    "    loss = f(params, x_i, y_i)\n",
    "    d_loss_wrt_params = ... # compute gradient\n",
    "    params -= learning_rate * d_loss_wrt_params\n",
    "    if <stopping condition is met>:\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variant that we recommend for deep learning is a further twist on stochastic gradient descent using so-called “minibatches”. Minibatch SGD (MSGD) works identically to SGD, except that we use more than one training example to make each estimate of the gradient. This technique reduces variance in the estimate of the gradient, and often makes better use of the hierarchical memory organization in modern computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for (x_batch,y_batch) in train_batches:\n",
    "                            # imagine an infinite generator\n",
    "                            # that may repeat examples\n",
    "    loss = f(params, x_batch, y_batch)\n",
    "    d_loss_wrt_params = ... # compute gradient using theano\n",
    "    params -= learning_rate * d_loss_wrt_params\n",
    "    if <stopping condition is met>:\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All code-blocks above show pseudocode of how the algorithm looks like. Implementing such algorithm in Theano can be done as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minibatch Stochastic Gradient Descent\n",
    "\n",
    "# assume loss is a symbolic description of the loss function given\n",
    "# the symbolic variables params (shared variable), x_batch, y_batch;\n",
    "\n",
    "# compute gradient of loss with respect to params\n",
    "d_loss_wrt_params = T.grad(loss, params)\n",
    "\n",
    "# compile the MSGD step into a theano function\n",
    "updates = [(params, params - learning_rate * d_loss_wrt_params)]\n",
    "MSGD = theano.function([x_batch,y_batch], loss, updates=updates)\n",
    "\n",
    "for (x_batch, y_batch) in train_batches:\n",
    "    # here x_batch and y_batch are elements of train_batches and\n",
    "    # therefore numpy arrays; function MSGD also updates the params\n",
    "    print('Current loss is ', MSGD(x_batch, y_batch))\n",
    "    if stopping_condition_is_met:\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more to machine learning than optimization. When we train our model from data we are trying to prepare it to do well on new examples, not the ones it has already seen. The training loop above for MSGD does not take this into account, and may overfit the training examples. A way to combat overfitting is through regularization. There are several techniques for regularization; the ones we will explain here are L1/L2 regularization and early-stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 and L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 and L2 regularization involve adding an extra term to the loss function, which penalizes certain parameter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# symbolic Theano variable that represents the L1 regularization term\n",
    "L1  = T.sum(abs(param))\n",
    "\n",
    "# symbolic Theano variable that represents the squared L2 term\n",
    "L2 = T.sum(param ** 2)\n",
    "\n",
    "# the loss\n",
    "loss = NLL + lambda_1 * L1 + lambda_2 * L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early-Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early-stopping combats overfitting by monitoring the model’s performance on a validation set. A validation set is a set of examples that we never use for gradient descent, but which is also not a part of the test set. The validation examples are considered to be representative of future test examples. We can use them during training because they are not part of the test set. If the model’s performance ceases to improve sufficiently on the validation set, or even degrades with further optimization, then the heuristic implemented here gives up on much further optimization.\n",
    "\n",
    "The choice of when to stop is a judgement call and a few heuristics exist, but these tutorials will make use of a strategy based on a geometrically increasing amount of patience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# early-stopping parameters\n",
    "patience = 5000  # look as this many examples regardless\n",
    "patience_increase = 2     # wait this much longer when a new best is\n",
    "                              # found\n",
    "improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                               # considered significant\n",
    "validation_frequency = min(n_train_batches, patience/2)\n",
    "                              # go through this many\n",
    "                              # minibatches before checking the network\n",
    "                              # on the validation set; in this case we\n",
    "                              # check every epoch\n",
    "\n",
    "best_params = None\n",
    "best_validation_loss = numpy.inf\n",
    "test_score = 0.\n",
    "start_time = time.clock()\n",
    "\n",
    "done_looping = False\n",
    "epoch = 0\n",
    "while (epoch < n_epochs) and (not done_looping):\n",
    "    # Report \"1\" for first epoch, \"n_epochs\" for last epoch\n",
    "    epoch = epoch + 1\n",
    "    for minibatch_index in range(n_train_batches):\n",
    "\n",
    "        d_loss_wrt_params = ... # compute gradient\n",
    "        params -= learning_rate * d_loss_wrt_params # gradient descent\n",
    "\n",
    "        # iteration number. We want it to start at 0.\n",
    "        iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "        # note that if we do `iter % validation_frequency` it will be\n",
    "        # true for iter = 0 which we do not want. We want it true for\n",
    "        # iter = validation_frequency - 1.\n",
    "        if (iter + 1) % validation_frequency == 0:\n",
    "\n",
    "            this_validation_loss = ... # compute zero-one loss on validation set\n",
    "\n",
    "            if this_validation_loss < best_validation_loss:\n",
    "\n",
    "                # improve patience if loss improvement is good enough\n",
    "                if this_validation_loss < best_validation_loss * improvement_threshold:\n",
    "\n",
    "                    patience = max(patience, iter * patience_increase)\n",
    "                best_params = copy.deepcopy(params)\n",
    "                best_validation_loss = this_validation_loss\n",
    "\n",
    "        if patience <= iter:\n",
    "            done_looping = True\n",
    "            break\n",
    "\n",
    "# POSTCONDITION:\n",
    "# best_params refers to the best out-of-sample parameters observed during the optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
