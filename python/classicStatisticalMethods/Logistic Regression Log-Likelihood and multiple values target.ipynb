{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Logistic regression is a probabilistic, linear classifier. It is parametrized by a weight matrix W and a bias vector b. Classification is done by projecting an input vector onto a set of hyperplanes, each of which corresponds to a class. The distance from the input to a hyperplane reflects the probability that the input is a member of the corresponding class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, the probability that an input vector x is a member of a class i, a value of a stochastic variable Y, can be written as:\n",
    "\n",
    "##### P(Y=i|x, W,b) = softmax(W x + b) = exp(W_i x + b_i)/ sum_j (exp(W_j x + b_j)\n",
    "\n",
    "The modelâ€™s prediction y_{pred} is the class whose probability is maximal, specifically:\n",
    "\n",
    "##### y_pred = argmax_i P(Y=i|x,W,b)\n",
    "\n",
    "The code to do this in Theano is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
    "self.W = theano.shared(\n",
    "    value=numpy.zeros(\n",
    "(n_in, n_out),\n",
    "dtype=theano.config.floatX\n",
    "    ),\n",
    "    name='W',\n",
    "    borrow=True\n",
    ")\n",
    "# initialize the biases b as a vector of n_out 0s\n",
    "self.b = theano.shared(\n",
    "    value=numpy.zeros(\n",
    "(n_out,),\n",
    "dtype=theano.config.floatX\n",
    "    ),\n",
    "    name='b',\n",
    "    borrow=True\n",
    ")\n",
    "\n",
    "# symbolic expression for computing the matrix of class-membership\n",
    "# probabilities\n",
    "# Where:\n",
    "# W is a matrix where column-k represent the separation hyperplane for\n",
    "# class-k\n",
    "# x is a matrix where row-j  represents input training sample-j\n",
    "# b is a vector where element-k represent the free parameter of\n",
    "# hyperplane-k\n",
    "self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "\n",
    "# symbolic description of how to compute prediction as class whose\n",
    "# probability is maximal\n",
    "self.y_pred = T.argmax(self.p_y_given_x, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the parameters of the model must maintain a persistent state throughout training, we allocate shared variables for W,b. This declares them both as being symbolic Theano variables, but also initializes their contents. The dot and softmax operators are then used to compute the vector P(Y|x, W,b). The result p_y_given_x is a symbolic variable of vector-type.\n",
    "\n",
    "To get the actual model prediction, we can use the T.argmax operator, which will return the index at which p_y_given_x is maximal (i.e. the class with maximum probability).\n",
    "\n",
    "Now of course, the model we have defined so far does not do anything useful yet, since its parameters are still in their initial state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning optimal model parameters involves minimizing a loss function. In the case of multi-class logistic regression, it is very common to use the negative log-likelihood as the loss. This is equivalent to maximizing the likelihood of the data set D under the model parameterized by \\theta. Let us first start by defining the likelihood L and loss l.\n",
    "\n",
    "While entire books are dedicated to the topic of minimization, gradient descent is by far the simplest method for minimizing arbitrary non-linear functions. This tutorial will use the method of stochastic gradient method with mini-batches (MSGD). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Theano code defines the (symbolic) loss for a given minibatch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
    "# number of examples (call it n) in the minibatch\n",
    "# T.arange(y.shape[0]) is a symbolic vector which will contain\n",
    "# [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
    "# Log-Probabilities (call it LP) with one row per example and\n",
    "# one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
    "# v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
    "# LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
    "# the mean (across minibatch examples) of the elements in v,\n",
    "# i.e., the mean log-likelihood across the minibatch.\n",
    "return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Creating a LogisticRegression class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the tools we need to define a LogisticRegression class, which encapsulates the basic behaviour of logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    \"\"\"Multi-class Logistic Regression Class\n",
    "\n",
    "    The logistic regression is fully described by a weight matrix :math:`W`\n",
    "    and bias vector :math:`b`. Classification is done by projecting data\n",
    "    points onto a set of hyperplanes, the distance to which is used to\n",
    "    determine a class membership probability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        \"\"\" Initialize the parameters of the logistic regression\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: symbolic variable that describes the input of the\n",
    "                      architecture (one minibatch)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: number of input units, the dimension of the space in\n",
    "                     which the datapoints lie\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of output units, the dimension of the space in\n",
    "                      which the labels lie\n",
    "\n",
    "        \"\"\"\n",
    "        # start-snippet-1\n",
    "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
    "        self.W = theano.shared(\n",
    "            value=np.zeros(\n",
    "                (n_in, n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        # initialize the biases b as a vector of n_out 0s\n",
    "        self.b = theano.shared(\n",
    "            value=np.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        # symbolic expression for computing the matrix of class-membership\n",
    "        # probabilities\n",
    "        # Where:\n",
    "        # W is a matrix where column-k represent the separation hyperplane for\n",
    "        # class-k\n",
    "        # x is a matrix where row-j  represents input training sample-j\n",
    "        # b is a vector where element-k represent the free parameter of\n",
    "        # hyperplane-k\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "\n",
    "        # symbolic description of how to compute prediction as class whose\n",
    "        # probability is maximal\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        # end-snippet-1\n",
    "\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        # keep track of model input\n",
    "        self.input = input\n",
    "\n",
    "    def negative_log_likelihood(self, y):\n",
    "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
    "        of this model under a given target distribution.\n",
    "\n",
    "        .. math::\n",
    "\n",
    "            \\frac{1}{|\\mathcal{D}|} \\mathcal{L} (\\theta=\\{W,b\\}, \\mathcal{D}) =\n",
    "            \\frac{1}{|\\mathcal{D}|} \\sum_{i=0}^{|\\mathcal{D}|}\n",
    "                \\log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\\\\n",
    "            \\ell (\\theta=\\{W,b\\}, \\mathcal{D})\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "\n",
    "        Note: we use the mean instead of the sum so that\n",
    "              the learning rate is less dependent on the batch size\n",
    "        \"\"\"\n",
    "        # start-snippet-2\n",
    "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
    "        # number of examples (call it n) in the minibatch\n",
    "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
    "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
    "        # Log-Probabilities (call it LP) with one row per example and\n",
    "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
    "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
    "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
    "        # the mean (across minibatch examples) of the elements in v,\n",
    "        # i.e., the mean log-likelihood across the minibatch.\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "        # end-snippet-2\n",
    "\n",
    "    def errors(self, y):\n",
    "        \"\"\"Return a float representing the number of errors in the minibatch\n",
    "        over the total number of examples of the minibatch ; zero one\n",
    "        loss over the size of the minibatch\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "        \"\"\"\n",
    "\n",
    "        # check if y has same dimension of y_pred\n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError(\n",
    "                'y should have the same shape as self.y_pred',\n",
    "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
    "            )\n",
    "        # check if y is of the correct datatype\n",
    "        if y.dtype.startswith('int'):\n",
    "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
    "            # represents a mistake in prediction\n",
    "            return T.mean(T.neq(self.y_pred, y))\n",
    "        else:\n",
    "            raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate symbolic variables for input (x and y represent a\n",
    "# minibatch)\n",
    "x = T.matrix('x')  # data, presented as rasterized images\n",
    "y = T.ivector('y')  # labels, presented as 1D vector of [int] labels\n",
    "\n",
    "# construct the logistic regression class\n",
    "# Each MNIST image has size 28*28\n",
    "classifier = LogisticRegression(input=x, n_in=28 * 28, n_out=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by allocating symbolic variables for the training inputs x and their corresponding classes y. Note that x and y are defined outside the scope of the LogisticRegression object. Since the class requires the input to build its graph, it is passed as a parameter of the __init__ function. This is useful in case you want to connect instances of such classes to form a deep network. The output of one layer can be passed as the input of the layer above. (This tutorial does not build a multi-layer network, but this code will be reused in future tutorials that do.)\n",
    "\n",
    "Finally, we define a (symbolic) cost variable to minimize, using the instance method classifier.negative_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the cost we minimize during training is the negative log likelihood of\n",
    "# the model in symbolic format\n",
    "cost = classifier.negative_log_likelihood(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement MSGD in most programming languages (C/C++, Matlab, Python), one would start by manually deriving the expressions for the gradient of the loss with respect to the parameters: in this case the partial derivates of the negative log loss for W and b.This can get pretty tricky for complex models, as expressions can get fairly complex, especially when taking into account problems of numerical stability.\n",
    "\n",
    "With Theano, this work is greatly simplified. It performs automatic differentiation and applies certain math transforms to improve numerical stability.\n",
    "\n",
    "To get the gradients in Theano, simply do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_W = T.grad(cost=cost, wrt=classifier.W)\n",
    "g_b = T.grad(cost=cost, wrt=classifier.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g_W and g_b are symbolic variables, which can be used as part of a computation graph. The function train_model, which performs one step of gradient descent, can then be defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify how to update the parameters of the model as a list of\n",
    "    # (variable, update expression) pairs.\n",
    "updates = [(classifier.W, classifier.W - learning_rate * g_W),\n",
    "               (classifier.b, classifier.b - learning_rate * g_b)]\n",
    "\n",
    "    # compiling a Theano function `train_model` that returns the cost, but in\n",
    "    # the same time updates the parameter of the model based on the rules\n",
    "    # defined in `updates`\n",
    "train_model = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=cost,\n",
    "    updates=updates,\n",
    "    givens={\n",
    "        x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "        y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "updates is a list of pairs. In each pair, the first element is the symbolic variable to be updated in the step, and the second element is the symbolic function for calculating its new value. Similarly, givens is a dictionary whose keys are symbolic variables and whose values specify their replacements during the step. The function train_model is then defined such that:\n",
    "\n",
    "    the input is the mini-batch index index that, together with the batch size (which is not an input since it is fixed) defines x with corresponding labels y\n",
    "    the return value is the cost/loss associated with the x, y defined by the index\n",
    "    on every function call, it will first replace x and y with the slices from the training set specified by index. Then, it will evaluate the cost associated with that minibatch and apply the operations defined by the updates list.\n",
    "\n",
    "Each time train_model(index) is called, it will thus compute and return the cost of a minibatch, while also performing a step of MSGD. The entire learning algorithm thus consists in looping over all examples in the dataset, considering all the examples in one minibatch at a time, and repeatedly calling the train_model function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Prediction Using a Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict():\n",
    "    \"\"\"\n",
    "    An example of how to load a trained model and use it\n",
    "    to predict labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # load the saved model\n",
    "    classifier = pickle.load(open('best_model.pkl'))\n",
    "\n",
    "    # compile a predictor function\n",
    "    predict_model = theano.function(\n",
    "        inputs=[classifier.input],\n",
    "        outputs=classifier.y_pred)\n",
    "\n",
    "    # We can test it on some examples from test test\n",
    "    dataset='mnist.pkl.gz'\n",
    "    datasets = load_data(dataset)\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "    test_set_x = test_set_x.get_value()\n",
    "\n",
    "    predicted_values = predict_model(test_set_x[:10])\n",
    "    print(\"Predicted values for the first 10 examples in test set:\")\n",
    "    print(predicted_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
