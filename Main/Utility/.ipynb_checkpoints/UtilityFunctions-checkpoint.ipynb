{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonio/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/antonio/.local/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import time\n",
    "%matplotlib inline\n",
    "from sklearn import preprocessing\n",
    "import sklearn.feature_selection as selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from collections import defaultdict\n",
    "import xgboost as xgb\n",
    "import multiprocessing as mp\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nameCleaning(df):\n",
    "    # Custom cleaning\n",
    "    df.columns = [re.sub(\"[\\\\. \\\\(\\\\)\\\\/]+\", \"_\", elem) for elem in df.columns]\n",
    "    df.columns = [re.sub(\"-\", \"_\", elem) for elem in df.columns]\n",
    "    df.columns = [re.sub(\"'\", \"\", elem) for elem in df.columns]\n",
    "    df.columns = [re.sub(\",\", \"_\", elem) for elem in df.columns]\n",
    "    df.columns = [re.sub(\":\", \"_\", elem) for elem in df.columns]\n",
    "    df.columns = [re.sub(\"<\", \"MIN\", elem) for elem in df.columns]\n",
    "    df.columns = [re.sub(\">\", \"MAG\", elem) for elem in df.columns]\n",
    "    df.columns = [re.sub(\"&\", \"E\", elem) for elem in df.columns]\n",
    "    df.columns = [re.sub(\"Â°\", \"\", elem) for elem in df.columns]\n",
    "    df.columns = [re.sub(\"%\", \"PERC\", elem) for elem in df.columns]\n",
    "    df.columns = [re.sub(\"\\\\+\", \"_\", elem) for elem in df.columns]\n",
    "    # String upper\n",
    "    df.columns = [elem.upper() for elem in df.columns]\n",
    "    # Trim\n",
    "    df.columns = [elem.strip() for elem in df.columns]\n",
    "    # Cut recurring underscore\n",
    "    df.columns = [re.sub(\"_+\", \"_\", elem) for elem in df.columns]\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hist(df, col, nbins=None, kde=False):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - kde -> kernel density estimate, a way to esimate the probability density function of a \n",
    "                 random variable\n",
    "    \"\"\"\n",
    "    sns.set_style('dark')\n",
    "    sns.utils.axlabel(col, 'Frequency')\n",
    "    sns.distplot(df[col], bins=nbins,kde=kde)\n",
    "\n",
    "def compare_parametrical_distribution(df, col, nbins=None, par_distr = stats.gamma):\n",
    "    \"\"\"\n",
    "    Fit a parametric distribution to a dataset and visually evaluate how closely it corresponds\n",
    "    to the observed data (default gamma)\n",
    "    \"\"\"\n",
    "    sns.distplot(df[col], kde=False, bins=nbins, fit=stats.gamma)\n",
    "    \n",
    "def make_boxplot(df, quant_col, qual_col=None):\n",
    "    sns.set_style('dark')\n",
    "    sns.boxplot(df[qual_col],df[quant_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairplot(df, col_subset=\"all\", hue=None, diag_kind=\"hist\",size=2.5):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "        - hue -> categorical feature (often the target variable)\n",
    "        - diag_kind -> hist, kde\n",
    "    \"\"\"\n",
    "    if col_subset == \"all\":\n",
    "        sns.pairplot(df,hue=hue,diag_kind=diag_kind,size=size)\n",
    "    else:\n",
    "        sns.pairplot(df[col_subset],hue=hue,diag_kind=diag_kind,size=size)\n",
    "        \n",
    "def make_scatterplot(df, col_x, col_y,size=6):\n",
    "    sns.jointplot(x=col_x, y=col_y, data=df,size=size)\n",
    "    \n",
    "def make_scatterplot_with_hue(df, col_x, col_y,hue=None,size=5):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - hue -> categorical feature (often the target variable)\n",
    "    \"\"\"\n",
    "    sns.FacetGrid(df, hue=hue,size=size) \\\n",
    "   .map(plt.scatter, col_x, col_y) \\\n",
    "   .add_legend()\n",
    "def make_hexbin_plot(df, col_x, col_y,size=6):\n",
    "    \"\"\"\n",
    "    The bivariate analogue of a histogram is known as a “hexbin” plot, because it shows the \n",
    "    counts of observations that fall within hexagonal bins.\n",
    "    \"\"\"\n",
    "    with sns.axes_style(\"white\"):\n",
    "        sns.jointplot(x=df[col_x], y=df[col_y], kind=\"hex\", color=\"k\",size=size);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_missing_values(df):\n",
    "    missing_values = np.sum(df.isnull())/df.shape[0]\n",
    "    return pd.DataFrame(missing_values.rename(\"Missing Values\"))\n",
    "\n",
    "def descriptive(df, n):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        n -> number of example values that you want to see in the output\n",
    "    \"\"\"\n",
    "    df_describe = df.describe(include=\"all\").T\n",
    "    df_describe[\"Type\"] = df.dtypes\n",
    "    df_describe = df_describe[list(df_describe.columns)[-1:] + list(df_describe.columns)[0:-1]]\n",
    "    df_describe[\"First \" + str(n) + \" values\"] = df_describe.index.map(lambda x: df[x].dropna().unique()[:5])\n",
    "    return df_describe\n",
    "\n",
    "def find_duplicates(df, cols):\n",
    "    \"\"\"\n",
    "    It returns the rows of the dataframe with duplicates\n",
    "    \"\"\"\n",
    "    return df[df[cols].duplicated(keep = False)]\n",
    "\n",
    "def first_n_unique_values(df, col, n):\n",
    "    \"\"\"\n",
    "    It Returns the first n unique values of a given column of the dataframe df\n",
    "    \"\"\"\n",
    "    unique_values = df[col][~df[col].isnull()].unique().tolist()\n",
    "    if len(unique_values) < n:\n",
    "        return unique_values\n",
    "    else:\n",
    "        return unique_values[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_na(df, d):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - d -> dictionary with column name and value to substitute missing values\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    for col in d.keys():\n",
    "        df_copy[col] = df_copy[col].fillna(d[col])\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variables Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(df, cols = None):\n",
    "    \"\"\"\n",
    "    It returns the dataframe with encoded features (even with missing values) and the encoder, \n",
    "    if the list of columns is not provided then it encode all the object type features\n",
    "    \"\"\"\n",
    "    if not cols:\n",
    "        cols = list()\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == \"O\":\n",
    "                cols.append(col)\n",
    "            else:\n",
    "                pass\n",
    "    dict_le = defaultdict(preprocessing.LabelEncoder)\n",
    "    df_new = df.copy()\n",
    "    for col in cols:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        df_new = fill_na(df_new, {col: \"NaN\"})\n",
    "        df_new[col] = le.fit_transform(df_new[col])\n",
    "        dict_le[col] = le\n",
    "    return df_new, dict_le\n",
    "\n",
    "def decoding(df, cols, le):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - le -> LabelEncoder from the encoding function\n",
    "    \"\"\"\n",
    "    df_new = df.copy()\n",
    "    for col in cols:\n",
    "        df_new[col] = le.inverse_transform(df_new[col])\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization & Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(df, cols):\n",
    "    \"\"\"\n",
    "    It Takes the columns as a list and give back the dataframe with the standardized columns\n",
    "    \"\"\"\n",
    "    df_new = df.copy()\n",
    "    for col in cols:\n",
    "        df_new[col] = (df_new[col] - np.mean(df_new[col]))/np.std(df_new[col])\n",
    "    return df_new\n",
    "\n",
    "def normalize(df, cols):\n",
    "    \"\"\"\n",
    "    It takes the columns as a list and give back the dataframe with the normalized columns\n",
    "    \"\"\"\n",
    "    df_new = df.copy()\n",
    "    for col in cols:\n",
    "        df_new[col] = (df_new[col] - np.min(df_new[col]))/(np.max(df_new[col]) - np.min(df_new[col]))\n",
    "        return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binning(col, cut_points, labels=None):\n",
    "    \"\"\"\n",
    "    Binning a single column  \n",
    "    \"\"\"\n",
    "    minval = col.min()\n",
    "    max_val = col.max()\n",
    "    break_points = [minval] + cut_points + [maxval]\n",
    "    if not labels:\n",
    "        labels = range(len(cut_points)+1)\n",
    "    colBin = pd.cut(col,bins=break_points,labels=labels,include_lowest=True)\n",
    "    return colBinimport\n",
    "\n",
    "def coding(col, codeDict):\n",
    "    \"\"\"\n",
    "    It returns the column given with a new one with the old value (key) replaced by the \n",
    "    value given in the dictionary\n",
    "    \"\"\"\n",
    "    colCoded = pd.Series(col, copy=True)\n",
    "    for key, value in codeDict.items():\n",
    "        colCoded.replace(key, value, inplace=True)\n",
    "    return colCoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction between best Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interaction_features(df, cols):\n",
    "    \"\"\"\n",
    "    Returns a dict of name and numpy array with the new features\n",
    "    \"\"\"\n",
    "    results = dict()\n",
    "    for index in range(len(cols)):\n",
    "        index_1 = index + 1\n",
    "        while index_1 < len(cols): \n",
    "            results[cols[index]+\"x\"+cols[index_1]] = df[cols[index]]*df[cols[index_1]]\n",
    "            if np.sum(df[cols[index_1]] == 0) == 0:\n",
    "                results[cols[index]+\"/\"+cols[index_1]] = np.array(df[cols[index]])/np.array(df[cols[index_1]])\n",
    "            index_1 += 1\n",
    "    return results            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby and Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def many_aggregation_functions_groupby(df, keys, f):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - keys -> list of columns\n",
    "        - f -> dictionary in the form {variable: {\"variable_name_mean\":np.mean,\n",
    "               \"variable_name_min\":np.min}} (does not work for single aggregation function)\n",
    "    \"\"\"\n",
    "    df_gb = df.groupby(by=keys).agg(f).reset_index()\n",
    "    df_gb.columns = df_gb.columns.droplevel()\n",
    "    return df_gb\n",
    "\n",
    "def groupby_function(df, key, f):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - keys -> list of columns\n",
    "        - f -> dictionary in the form {variable: {\"variable_name_mean\":np.mean,\n",
    "               \"variable_name_min\":np.min}} (does not work for single aggregation function)\n",
    "    \"\"\"\n",
    "    df_gb = df.groupby(by=key).agg(f).reset_index()\n",
    "    return df_gb\n",
    "\n",
    "def parallelized_groupby(df, keys, f):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - keys -> list of columns\n",
    "        - f -> dictionary in the form {variable: {\"variable_name_mean\":np.mean,\n",
    "               \"variable_name_min\":np.min}} (does not work for single aggregation function)\n",
    "    \"\"\"\n",
    "    grouped = df.groupby(keys)\n",
    "    df_s = []\n",
    "    for index in grouped.indices.values():\n",
    "        df_s.append(df.iloc[index])\n",
    "    func = delayed(groupby_function)\n",
    "    res = Parallel(n_jobs=4, verbose=10)(func(s, keys, f) for s in df_s)\n",
    "    return pd.concat(res)\n",
    "\n",
    "\n",
    "def apply_df(df, f): \n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        - df -> dataframe\n",
    "        - f -> function that takes row as an input and do some operation along row\n",
    "    \"\"\"\n",
    "    return df.apply(f, axis=1) \n",
    "\n",
    "def parallelized_apply(df, apply_func, f, num_partitions):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - df -> dataframe\n",
    "        - apply_func -> function that takes row and f as an input and return df.apply()\n",
    "        - f -> input of the apply_df\n",
    "    \"\"\"\n",
    "    func = delayed(apply_func) \n",
    "    res = Parallel(n_jobs=4, verbose=10)(func(s, f=f) for s in np.array_split(df, num_partitions)) \n",
    "    return pd.concat(res) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_univariate(X_train, y_train, n=None, score_function=selection.chi2):\n",
    "    \"\"\"\n",
    "    Returns the new X with the best n columns\n",
    "    Inputs:\n",
    "        - n -> number of feature to select (in case of Select K best, otherwise None)\n",
    "        - score_function -> chi-square default (classification) other here:\n",
    "                            http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest\n",
    "    \"\"\"\n",
    "    return selection.SelectKBest(score_func=score_function, k=n).fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recursive_elimination(X, y, n, model, step=1):\n",
    "    \"\"\"\n",
    "    The Recursive Feature Elimination (RFE) method is a feature selection approach. \n",
    "    It works by recursively removing attributes and building a model on those attributes that \n",
    "    remain. It uses the model accuracy to identify which attributes (and combination of \n",
    "    attributes) contribute the most to predicting the target attribute.\n",
    "    \n",
    "    Inputs:\n",
    "        - n -> number of features to select\n",
    "        - model -> model used in order to find the best n variables\n",
    "        - step -> int or float, optional (default=1), if greater than or equal to 1, then \n",
    "                  `step` corresponds to the (integer) number of features to remove at each \n",
    "                  iteration.\n",
    "                  If within (0.0, 1.0), then `step` corresponds to the percentage\n",
    "                  (rounded down) of features to remove at each iteration.\n",
    "    Outputs:\n",
    "        - support (True and False)\n",
    "        - ranking\n",
    "    \"\"\"\n",
    "    rfe = selection.RFE(model, n_features_to_select=n)\n",
    "    return rfe.fit(X, y).support_, rfe.fit(X, y).ranking_ \n",
    "\n",
    "def recursive_elimination_CV(X, y, model, step = 1, nCvFold = 3, score = \"accuracy\"):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - score: string (\"accuracy\" default) or a scorer callable object / function with signature \n",
    "          scorer(estimator, X, y)\n",
    "    Output:\n",
    "        - rfecv object with the following attributes:\n",
    "            - n_features_ : The number of selected features with cross-validation\n",
    "            - support_ : the mask of selected features.\n",
    "            - ranking_ : the feature ranking, such that ranking_[i] corresponds to the ranking position of the i-th feature. Selected (i.e., estimated best) features are assigned rank 1.\n",
    "            - grid_scores_ : the cross-validation scores such that grid_scores_[i] corresponds to the CV score of the i-th subset of features.\n",
    "            - estimator_ : the external estimator fit on the reduced dataset.\n",
    "    \"\"\"\n",
    "    rfecv = RFECV(estimator=model, step=step, cv=StratifiedKFold(nCvFold),\n",
    "              scoring=score)\n",
    "    rfecv.fit(X, y)\n",
    "    return rfecv\n",
    "\n",
    "def plot_RFECV(rfecv):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - rfecv: rfecv object RFECV after fit\n",
    "    \"\"\"\n",
    "    print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "    plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_importance_trees(X, y, n, model):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - n: number of feature to select\n",
    "    Output:\n",
    "        - X_selected_features: X with the selected features\n",
    "    \"\"\"\n",
    "    model.fit(X,y)\n",
    "    feature_importance = pd.DataFrame({\"Feature\": X.columns, \\\n",
    "                                       \"Feature_Importance\":model.feature_importances_})\n",
    "    feature_importance.sort(columns=\"Feature_Importance\",ascending=False, inplace=True)\n",
    "    return X[feature_importance.Feature.tolist()[:n]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training-Validation-Test Set Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_df(df, train_size = 0.67, random_state=42):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "        - df_train, df_test\n",
    "    \"\"\"\n",
    "    df_train, df_test = train_test_split(df, train_size=train_size, random_state=random_state)\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def train_test_split_X_y(X, y, train_size = 0.67, random_state=42):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "        - X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_CV(X_train, y_train, model, parameters, n_cv= 3, scoring=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - parameters: dictionary with name of the parameters as keys and a list of possible\n",
    "                      values as values of the dictionary.\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    clf = GridSearchCV(model, parameters, cv=StratifiedKFold(n_splits=n_cv, shuffle=True, random_state=101).get_n_splits(X_train, y_train),\n",
    "                       verbose=0, n_jobs=-1).fit(X_train, y_train)\n",
    "    print('\\nRFR Fitted in %0.3f[s]' %(time.time() - t0))\n",
    "    print(\"Best score: \" + str(clf.best_score_))\n",
    "    clf_best = clf.best_estimator_\n",
    "    return clf_best\n",
    "\n",
    "def early_stopping_CV(X_train, y_train, params, n_boost_round = 3000, n_folds = 5, metrics = \"error\", metrics_min = True, early_stopping_rounds = 100):\n",
    "    \"\"\"\n",
    "    Tuning the early stopping parameter using Cross Validation\n",
    "    Output:\n",
    "        - cv_xgb: pandas Dataframe with all the rounds\n",
    "        - final_xgb: trained xgboost with the optimal number of boost_round\n",
    "    \"\"\"\n",
    "    xgdmat = xgb.DMatrix(X_train, y_train)\n",
    "    cv_xgb = xgb.cv(params = params, dtrain = xgdmat, num_boost_round = n_boost_round, nfold = n_folds,\n",
    "                metrics = [metrics], # Make sure you enter metrics inside a list or you may encounter issues!\n",
    "                early_stopping_rounds = early_stopping_rounds) # Look for early stopping that minimizes error\n",
    "    if metrics_min == True:\n",
    "        best_boost_rounds = cv_xgb[cv_xgb.iloc[:,0] == cv_xgb.iloc[:,0].min()].index[0]\n",
    "    else:\n",
    "        best_boost_rounds = cv_xgb[cv_xgb.iloc[:,0] == cv_xgb.iloc[:,0].max()].index[0]  \n",
    "    final_xgb = xgb.train(params, xgdmat, num_boost_round = best_boost_rounds)\n",
    "    print(xgb.plot_importance(final_xgb))\n",
    "    return cv_xgb, final_xgb\n",
    "\n",
    "def cross_validation(X_train, y_train, model, n_folds, score=metrics.accuracy_score):\n",
    "    \"\"\"\n",
    "    Function that returns the scores of the n_folds iteration of the model training/prediction, if you take the mean\n",
    "    you have an estimation of the performance of the model\n",
    "    \"\"\"\n",
    "    scores   = []\n",
    "    cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=101).split(X_train, y_train)\n",
    "    for i, (train, test) in enumerate(cv):\n",
    "        y_pred = model.fit(X.loc[train], y.loc[train]).predict(X.loc[test])\n",
    "        scores.append(score(y[test], y_pred))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrics(y_test, y_hat):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - y_hat: y predicted by the model\n",
    "    Output:\n",
    "        - classification report: report with precision, recall and f1-score \n",
    "        - confusion matrix: y_hat in the rows and y_test in the columns\n",
    "    \"\"\"\n",
    "    return classification_report(y_test, y_hat), pd.crosstab(y_hat, y_test)\n",
    "\n",
    "def plot_ROC_curve(y_test, y_scores, labels):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - y_scores: list of y_score from different classifiers\n",
    "        - labels: list of labels of the different classifiers\n",
    "    It prints the ROC of every classifier in the same axis\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # add a straight line representing a random model \n",
    "    for index in range(len(y_scores)):\n",
    "        # false positive and true positive rate for each class\n",
    "        fpr, tpr, _ = metrics.roc_curve(y_test, y_scores[index])\n",
    "        # area under the curve (auc) for each class\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "        label = labels[index]\n",
    "        plt.plot(fpr, tpr, label='ROC curve of {0} (area = {1:0.2f})'.format(label, roc_auc))\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.title('Receiver Operating Characteristic for Iris data set')\n",
    "    plt.xlabel('False Positive Rate') # 1- specificity\n",
    "    plt.ylabel('True Positive Rate') # sensitivity\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def make_binary(x, thr):\n",
    "    \"\"\"\n",
    "    Transform a score x in binary based on the threshold thr\n",
    "    \"\"\"\n",
    "    if x >= thr:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def threshold_classification_metrics(y_test, y_score):\n",
    "    \"\"\"\n",
    "    It works only for binary classification.\n",
    "\n",
    "    Output:\n",
    "        - df: dataframe with the main scores for classification for every possible threshold\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"Threshold\": np.arange(0,1.01,0.01)})\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_score)\n",
    "    df[\"Roc_auc\"] = metrics.auc(fpr, tpr)\n",
    "    df[\"Recall\"] = df[\"Threshold\"].map(lambda thr: metrics.recall_score(y_test, pd.Series(y_score).map(lambda x: make_binary(x, thr))))\n",
    "    df[\"Precision\"] = df[\"Threshold\"].map(lambda thr: metrics.precision_score(y_test, pd.Series(y_score).map(lambda x: make_binary(x, thr))))\n",
    "    df[\"F-1 Measure\"] = df[\"Threshold\"].map(lambda thr: metrics.f1_score(y_test, pd.Series(y_score).map(lambda x: make_binary(x, thr))))\n",
    "    df[\"Kappa Score\"] = df[\"Threshold\"].map(lambda thr: metrics.cohen_kappa_score(y_test, pd.Series(y_score).map(lambda x: make_binary(x, thr))))\n",
    "    return df\n",
    "    \n",
    "def from_Percentile_to_Decile(x):\n",
    "    \"\"\"\n",
    "    Function used in the lift_raw function\n",
    "    \"\"\"\n",
    "    for index in [x*0.10 for x in range(1, 11)]:\n",
    "        if x <= index:\n",
    "            return str(round(index - 0.10,2)) + \" - \" + str(round(index,2))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "def lift_raw(y_true, y_score):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "        - df: dataframe with Deciles and Percentiles of the y_score for every observation\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"y_true\": y_true, \"y_score\": y_score}).sort(columns=\"y_score\", ascending=False)\n",
    "    df[\"Percentile\"] = np.arange(df.shape[0], 0, -1) / df.shape[0]\n",
    "    df[\"Decile\"] = df[\"Percentile\"].map(lambda x: from_Percentile_to_Decile(x))\n",
    "    return df    \n",
    "\n",
    "def lift(y_true, y_score):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "        - df: dataframe with the mean y_true and the lift score for every Decile of the y_score \n",
    "    \"\"\"\n",
    "    df = lift_raw(y_true, y_score)\n",
    "    df = df.groupby(by=\"Decile\").mean().reset_index()\n",
    "    df.drop(\"Percentile\",axis=1,inplace=True)\n",
    "    df[\"Overall\"] = np.sum(y_true)/y_true.shape[0]\n",
    "    df[\"Lift\"] = df[\"y_true\"] / df[\"Overall\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Best n for every different Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking(X_train, y_train, X_test, y_test, models, n_folds = 10):\n",
    "    \"\"\"\n",
    "    Stacking function that returns the dataset with the prediction of the different models for the train and for the test\n",
    "    set.\n",
    "    \"\"\"\n",
    "    dataset_blend_train = np.zeros((X_train.shape[0], len(clfs)))\n",
    "    dataset_blend_test = np.zeros((X_test.shape[0], len(clfs)))\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=101).split(X_train, y_train)\n",
    "    dataset_blend_test_j = np.zeros((X_test.shape[0], n_folds))\n",
    "    e_skf = list(enumerate(skf))\n",
    "    for j, model in enumerate(models):\n",
    "        print(\"Model number \" + str(j) + \"\\n\", model)\n",
    "        for i, (train, test) in e_skf:\n",
    "            model.fit(X_train.loc[train], y_train.loc[train])\n",
    "            y_pred = model.predict_proba(X_train.loc[test])[:, 1]\n",
    "            dataset_blend_train[test, j] = y_pred\n",
    "            dataset_blend_test_j[:, i] = model.predict_proba(X_test)[:, 1]\n",
    "        dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)\n",
    "    return dataset_blend_train, dataset_blend_test"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
