{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERAL DATA ANALYSIS PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels define the problem and can be of different types, such as:\n",
    "\n",
    "- Single column, binary values (classification problem, one sample belongs to one class only and there are only two classes)\n",
    "- Single column, real values (regression problem, prediction of only one value)\n",
    "- Multiple column, binary values (classification problem, one sample belongs to one class, but there are more than two classes)\n",
    "- Multiple column, real values (regression problem, prediction of multiple values)\n",
    "- Multilabel (classification problem, one sample can belong to several classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any kind of machine learning problem, we must know how we are going to evaluate our results, or what the evaluation metric or objective is. \n",
    "For example in case of a **skewed binary classification problem** we generally choose area under the receiver operating characteristic curve (**ROC AUC** or simply **AUC**). \n",
    "In case of **multi-label or multi-class classification problems**, we generally choose **categorical cross-entropy or multiclass log loss** and **mean squared error** in case of **regression problems**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training-Validation Set Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that you've already split the data in training-test set, then you have to split the training set in the real training and validation set (or use cross-validation).\n",
    "The splitting of data into training and validation sets “must” be done according to labels. In case of any kind of classification problem, use stratified splitting. In python, you can do this using scikit-learn very easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "eval_size = 0.10\n",
    "kf = StratifiedKFold(y, round(1./eval_size))\n",
    "train_indices, valid_indices = next(iter(kf))\n",
    "X_train, y_train = X[train_indices], y[train_indices]\n",
    "X_val, y_val = X[valid_indices], y[valid_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of regression task, a simple K-Fold splitting should suffice. There are, however, some complex methods which tend to keep the distribution of labels same for both training and validation but they're not treat here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "eval_size = 0.10\n",
    "kf = KFold(len(y), round(1./eval_size))\n",
    "train_indices, valid_indices = next(iter(kf))\n",
    "X_train, y_train = X[train_indices], y[train_indices]\n",
    "X_val, y_val = X[valid_indices], y[valid_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eval_size is here setted as 0.10, but it depends on how many data you have.\n",
    "After the splitting of the data is done, leave this data out and don’t touch it. Any operations that are applied on training set must be saved and then applied to the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is identification of different variables in the data. There are usually three types of variables we deal with. Namely, **numerical variables**, **categorical variables** and **variables with text inside them**.\n",
    "\n",
    "Separate out the numerical variables first. These variables don’t need any kind of processing and thus we can start applying normalization and machine learning models to these variables.\n",
    "\n",
    "There are two ways in which we can handle **categorical data**:\n",
    "- Convert the categorical data to labels\n",
    "- Convert the labels to binary variables (one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the categorical data to labels\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lbl_enc = LabelEncoder\n",
    "lbl_enc.fit(X_train[categorical_features])\n",
    "X_train_cat = lbl_enc.transform(X_train[categorical_features])\n",
    "\n",
    "# One-hot Encoding, remember to convert categories to number first using LabelEncoder then use OneHotEncoder, if it's\n",
    "# important to have the name of the catagories, use pd.get_dummies()\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder\n",
    "ohe.fit(X_train[categorical_features])\n",
    "X_train_cat = ohe.transform(X_train[categorical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s formulate a general rule on handling **text variables**. We can combine all the text variables into one and then use some algorithms which work on text data and convert it to numbers.\n",
    "\n",
    "The text variables can be joined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data = list(X_train[text_features].apply(lambda row: \"%s %s\" %(row[\"column 1\"], row[\"column 2\"]), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use CountVectorizer or TfidfVectorizer on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "ctv = CountVectorizer()\n",
    "text_data_train = cvt.fit_transform(text_data_train)\n",
    "vocab = cvt.get_feature_names()\n",
    "text_data_valid = cvt.transform(text_data_valid)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfv = TfidfVectorizer()\n",
    "text_data_train = tfv.fit_transform(text_data_train)\n",
    "vocab = tvf.get_feature_names()\n",
    "text_data_valid = tfv.transform(text_data_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TfidfVectorizer performs better than the counts most of the time and I have seen that the following parameters for TfidfVectorizer work almost all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfv = TfidfVectorizer(min_df=3, max_features=None, strip_accents=\"unicode\", analyzer=\"word\", token_pattern=r'\\w{1,}', \\\n",
    "    ngram_range=(1,2), use_idf=1,smooth_idf=1,sublinear_tf=1,stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature stacker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can horizontally stack all the features before putting them through further processing by using numpy hstack or sparse hstack depending on wheter you have dense or sparse features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "# in case of dense data:\n",
    "X = np.hstack((x1,x2, ...))\n",
    "\n",
    "# in case of sparse data:\n",
    "X = sparse.hstack((x1,x2, ...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can also be achieved by FeatureUnion module in case there are other preprocessing steps such as pca or feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "skb = SelectKBest(k=1)\n",
    "combined_features = FeatureUnion([(\"pca\", pca), (\"skb\",skb)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once, we have stacked the features together, we can start applying machine learning models. At this stage only models you should go for should be ensemble tree based models. These models include:\n",
    "\n",
    "- RandomForestClassifier\n",
    "- RandomForestRegressor\n",
    "- ExtraTreesClassifier\n",
    "- ExtraTreesRegressor\n",
    "- XGBClassifier\n",
    "- XGBRegressor\n",
    "We cannot apply linear models to the above features since they are not normalized. To use linear models, one can use Normalizer or StandardScaler from scikit-learn.\n",
    "\n",
    "These normalization methods work only on dense features and don’t give very good results if applied on sparse features. Yes, one can apply StandardScaler on sparse matrices without using the mean (parameter: with_mean=False).\n",
    "\n",
    "If the above steps give a “good” model, we can go for optimization of hyperparameters and in case it doesn’t we can go for the following steps and improve our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECOMPOSITION METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steps include decomposition methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://5047-presscdn.pagely.netdna-cdn.com/wp-content/uploads/2016/07/abhishek_17.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"http://5047-presscdn.pagely.netdna-cdn.com/wp-content/uploads/2016/07/abhishek_17.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity, we will leave out LDA and QDA transformations. For high dimensional data, generally PCA is used decompose the data. For images start with 10-15 components and increase this number as long as the quality of result improves substantially. For other type of data, we select 50-60 components initially (we tend to avoid PCA as long as we can deal with the numerical data as it is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=12)\n",
    "pca.fit(xtrain)\n",
    "xtrain = pca.transform(xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **text data**, after conversion of text to sparse matrix, go for Singular Value Decomposition (SVD). A variation of SVD called TruncatedSVD can be found in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=12)\n",
    "svd.fit(xtrain)\n",
    "xtrain = svd.transform(xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of SVD components that generally work for TF-IDF or counts are between 120-200. Any number above this might improve the performance but not substantially and comes at the cost of computing power.\n",
    "\n",
    "After evaluating further performance of the models, we move to scaling of the datasets, so that we can evaluate linear models too. The normalized or scaled features can then be sent to the machine learning models or feature selection modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways in which feature selection can be achieved. One of the most common way is greedy feature selection (forward or backward). In greedy feature selection we choose one feature, train a model and evaluate the performance of the model on a fixed evaluation metric. We keep adding and removing features one-by-one and record performance of the model at every step. We then select the features which have the best evaluation score. One implementation of greedy feature selection with AUC as evaluation metric can be found here: https://github.com/abhishekkrthakur/greedyFeatureSelection. It must be noted that this implementation is not perfect and must be changed/modified according to the requirements.\n",
    "\n",
    "Other faster methods of feature selection include selecting best features from a model. We can either look at coefficients of a logit model or we can train a random forest to select best features and then use them later with other machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "clf.fit(X,y)\n",
    "X_selected = clt.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to keep low number of estimators and minimal optimization of hyper parameters so that you don’t overfit.\n",
    "\n",
    "The feature selection can also be achieved using Gradient Boosting Machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "params = {}\n",
    "\n",
    "model = xgb.train(params, dtrain, num_boost_round=100)\n",
    "sorted(model.get_fscore().items(),key=lambda t: -t[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular method for feature selection from positive sparse datasets is chi-2 based feature selection and we also have that implemented in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "skb = SelectKBest(chi2, k=20)\n",
    "skb.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use chi2 in conjunction with SelectKBest to select 20 features from the data. This also becomes a hyperparameter we want to optimize to improve the result of our machine learning models.\n",
    "\n",
    "Next (or intermediate) major step is model selection + hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generally use the following algorithms in the process of selecting a machine learning model:\n",
    "\n",
    "Classification:\n",
    "- Random Forest\n",
    "- GBM\n",
    "- Logistic Regression\n",
    "- Naive Bayes\n",
    "- Support Vector Machines\n",
    "- k-Nearest Neighbors\n",
    "\n",
    "Regression:\n",
    "- Random Forest\n",
    "- GBM\n",
    "- Linear Regression\n",
    "- Ridge\n",
    "- Lasso\n",
    "- SVR\n",
    "\n",
    "Which parameters should I optimize? How do I choose parameters closest to the best ones? \n",
    "Let’s break down the hyperparameters, model wise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://5047-presscdn.pagely.netdna-cdn.com/wp-content/uploads/2016/07/abhishek_24.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"http://5047-presscdn.pagely.netdna-cdn.com/wp-content/uploads/2016/07/abhishek_24.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RS* = Cannot say about proper values, go for Random Search in these hyperparameters."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {
    "290ed72d-cf6d-4229-a38b-2f0d2f4c008a": {
     "id": "290ed72d-cf6d-4229-a38b-2f0d2f4c008a",
     "prev": null,
     "regions": {
      "d6f7467c-11fb-47b7-9a93-6b6e4f6437d1": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d1ae9006-cc4c-4b5f-ab8f-670c670610cd",
        "part": "whole"
       },
       "id": "d6f7467c-11fb-47b7-9a93-6b6e4f6437d1"
      }
     }
    },
    "ccd575df-de34-45a1-acb9-39851ab5beb5": {
     "id": "ccd575df-de34-45a1-acb9-39851ab5beb5",
     "prev": "290ed72d-cf6d-4229-a38b-2f0d2f4c008a",
     "regions": {
      "a95f79cb-30ca-4108-a83d-2e1d1260d743": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "2669642f-ee74-476f-a30f-90a7666f2135",
        "part": "whole"
       },
       "id": "a95f79cb-30ca-4108-a83d-2e1d1260d743"
      }
     }
    }
   },
   "themes": {
    "default": "ab9937ac-3a89-4edb-a771-0312337916c6",
    "theme": {
     "6009a2df-cb82-4e42-820a-a6ef0389758c": {
      "backgrounds": {
       "dc7afa04-bf90-40b1-82a5-726e3cff5267": {
        "background-color": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "id": "dc7afa04-bf90-40b1-82a5-726e3cff5267"
       }
      },
      "id": "6009a2df-cb82-4e42-820a-a6ef0389758c",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         197,
         226,
         245
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "a": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c"
       },
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 8
       },
       "h2": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "li": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3.25
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
       "font-family": "Lato",
       "font-size": 4
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
